<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Stochastic Treatment Regimes | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown  and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Stochastic Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/acic2019-workshop/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/acic2019-workshop" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Stochastic Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Mark J. van der Laan, Alan E. Hubbard, Jeremy R. Coyle, Nima S. Hejazi, Ivana Malenica, Rachael V. Phillips" />


<meta name="date" content="2019-03-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="optimal-individualized-treatment-regimes.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-workshop"><i class="fa fa-check"></i>About this workshop</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-instructors"><i class="fa fa-check"></i>About the instructors</a><ul>
<li class="chapter" data-level="0.0.1" data-path="index.html"><a href="index.html#mark-j.-van-der-laan"><i class="fa fa-check"></i><b>0.0.1</b> Mark J. van der Laan</a></li>
<li class="chapter" data-level="0.0.2" data-path="index.html"><a href="index.html#alan-e.-hubbard"><i class="fa fa-check"></i><b>0.0.2</b> Alan E. Hubbard</a></li>
<li class="chapter" data-level="0.0.3" data-path="index.html"><a href="index.html#jeremy-r.-coyle"><i class="fa fa-check"></i><b>0.0.3</b> Jeremy R. Coyle</a></li>
<li class="chapter" data-level="0.0.4" data-path="index.html"><a href="index.html#nima-s.-hejazi"><i class="fa fa-check"></i><b>0.0.4</b> Nima S. Hejazi</a></li>
<li class="chapter" data-level="0.0.5" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i><b>0.0.5</b> Ivana Malenica</a></li>
<li class="chapter" data-level="0.0.6" data-path="index.html"><a href="index.html#rachael-v.-phillips"><i class="fa fa-check"></i><b>0.0.6</b> Rachael V. Phillips</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i><b>2</b> Literature</a></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a></li>
<li class="chapter" data-level="4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>4</b> Optimal Individualized Treatment Regimes</a><ul>
<li class="chapter" data-level="4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-objectives"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#introduction-to-optimal-individualized-interventions"><i class="fa fa-check"></i><b>4.2</b> Introduction to Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#data-structure-and-notation"><i class="fa fa-check"></i><b>4.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="4.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#defining-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>4.4</b> Defining the Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="4.4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment"><i class="fa fa-check"></i><b>4.4.1</b> Binary treatment</a></li>
<li class="chapter" data-level="4.4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment"><i class="fa fa-check"></i><b>4.4.2</b> Categorical treatment</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interpreting-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>4.5</b> Interpreting the Causal Effect of an Optimal Individualized Intervention</a></li>
<li class="chapter" data-level="4.6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-binary-treatment"><i class="fa fa-check"></i><b>4.6</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with Binary Treatment</a></li>
<li class="chapter" data-level="4.7" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-categorical-treatment"><i class="fa fa-check"></i><b>4.7</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with Categorical Treatment</a><ul>
<li class="chapter" data-level="4.7.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data"><i class="fa fa-check"></i><b>4.7.1</b> Simulated Data</a></li>
<li class="chapter" data-level="4.7.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3"><i class="fa fa-check"></i><b>4.7.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-the-mean-outcome-under-the-optimal-rule-with-q-learning"><i class="fa fa-check"></i><b>4.7.3</b> Learning the Mean Outcome under the Optimal Rule with Q-learning</a></li>
<li class="chapter" data-level="4.7.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects"><i class="fa fa-check"></i><b>4.7.4</b> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
<li class="chapter" data-level="4.7.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extensions-simpler-rules"><i class="fa fa-check"></i><b>4.7.5</b> Extensions: Simpler Rules</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extensions-variable-importance-analysis-with-optimal-individualized-interventions"><i class="fa fa-check"></i><b>4.8</b> Extensions: Variable Importance Analysis with Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="4.9" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercises"><i class="fa fa-check"></i><b>4.9</b> Exercises</a><ul>
<li class="chapter" data-level="4.9.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#basicsreview"><i class="fa fa-check"></i><b>4.9.1</b> Basics/Review</a></li>
<li class="chapter" data-level="4.9.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#using-the-ideas"><i class="fa fa-check"></i><b>4.9.2</b> Using the Ideas</a></li>
<li class="chapter" data-level="4.9.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#advanced"><i class="fa fa-check"></i><b>4.9.3</b> Advanced</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>5</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="5.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#learning-objectives-1"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#introduction-to-stochastic-interventions"><i class="fa fa-check"></i><b>5.2</b> Introduction to Stochastic Interventions</a></li>
<li class="chapter" data-level="5.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#background-on-stochastic-interventions"><i class="fa fa-check"></i><b>5.3</b> Background on Stochastic Interventions</a></li>
<li class="chapter" data-level="5.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#data-structure-and-notation-1"><i class="fa fa-check"></i><b>5.4</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="5.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.5</b> Defining the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="5.6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#interpreting-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.6</b> Interpreting the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="5.7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#evaluating-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.7</b> Evaluating the Causal Effect of a Stochastic Intervention</a><ul>
<li class="chapter" data-level="5.7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#simulate-data"><i class="fa fa-check"></i><b>5.7.1</b> Simulate Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>5.7.2</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="5.7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>5.7.3</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>5.8</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>5.8.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="5.8.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>5.8.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="5.8.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects-1"><i class="fa fa-check"></i><b>5.8.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="5.8.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>5.8.4</b> Inference with Marginal Structural Models</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises-1"><i class="fa fa-check"></i><b>5.9</b> Exercises</a><ul>
<li class="chapter" data-level="5.9.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#basicsreview-1"><i class="fa fa-check"></i><b>5.9.1</b> Basics/Review</a></li>
<li class="chapter" data-level="5.9.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#using-the-ideas-1"><i class="fa fa-check"></i><b>5.9.2</b> Using the Ideas</a></li>
<li class="chapter" data-level="5.9.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#advanced-1"><i class="fa fa-check"></i><b>5.9.3</b> Advanced</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stochastic-treatment-regimes" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Stochastic Treatment Regimes</h1>
<p><em>Nima Hejazi, Jeremy Coyle, Mark van der Laan</em></p>
<p>Updated: 2019-03-27</p>
<div id="learning-objectives-1" class="section level2">
<h2><span class="header-section-number">5.1</span> Learning Objectives</h2>
<!--- appears as "X.1: Learning Objectives" in the book, where X is the chapter
corresponding to stochastic interventions -->
<ol style="list-style-type: decimal">
<li>…</li>
<li>…</li>
<li>…</li>
<li>…</li>
<li>…</li>
</ol>
</div>
<div id="introduction-to-stochastic-interventions" class="section level2">
<h2><span class="header-section-number">5.2</span> Introduction to Stochastic Interventions</h2>
<p>Stochastic treatment regimes present a relatively simple manner in which to
assess the effects of continuous treatments by way of parameters that examine
the effects induced by the counterfactual shifting of the observed values of a
treatment of interest. Here, we present an implementation of a new algorithm for
computing targeted minimum loss-based estimates of treatment shift parameters
defined based on a shifting function <span class="math inline">\(d(A,W)\)</span>. For a technical presentation of
the algorithm, the interested reader is invited to consult <span class="citation">Díaz and van der Laan (<a href="#ref-diaz2018stochastic">2018</a>)</span>.
For additional background on Targeted Learning and previous work on stochastic
treatment regimes, please consider consulting <span class="citation">van der Laan and Rose (<a href="#ref-vdl2011targeted">2011</a>)</span>,
<span class="citation">van der Laan and Rose (<a href="#ref-vdl2018targeted">2018</a>)</span>, and <span class="citation">Díaz and van der Laan (<a href="#ref-diaz2012population">2012</a>)</span>.</p>
</div>
<div id="background-on-stochastic-interventions" class="section level2">
<h2><span class="header-section-number">5.3</span> Background on Stochastic Interventions</h2>
<p>TODO</p>
</div>
<div id="data-structure-and-notation-1" class="section level2">
<h2><span class="header-section-number">5.4</span> Data Structure and Notation</h2>
<p>Consider <span class="math inline">\(n\)</span> observed units <span class="math inline">\(O_1, \ldots, O_n\)</span>, where each random variable <span class="math inline">\(O = (W, A, Y)\)</span> corresponds to a single observational unit. As discussed before, <span class="math inline">\(W\)</span>
are baseline covariates, <span class="math inline">\(A\)</span> is a continuous-valued intervention, and <span class="math inline">\(Y\)</span> an
outcome of interest. We will consider a simple stochastic intervention that
considers counterfactual interventions defined through scalar shifts of the
observed/natural value of the intervention <span class="math inline">\(A\)</span>. We’ll start by considering a
simple additive shift <span class="math inline">\(d(a,w) = a + \delta\)</span>, assuming support a.e. of
<span class="math inline">\(\mathbb{P}(A \mid W)\)</span>. To ease violations of the positivity assumption, we may
consider extensions where <span class="math inline">\(a \leq u(w) - \delta\)</span> or <span class="math inline">\(d(a, w) = a\)</span> if <span class="math inline">\(a \geq u(w) - \delta\)</span>.</p>
</div>
<div id="defining-the-causal-effect-of-a-stochastic-intervention" class="section level2">
<h2><span class="header-section-number">5.5</span> Defining the Causal Effect of a Stochastic Intervention</h2>
<p>Likelihood Factorization for the Full Data:
Let <span class="math inline">\(q_{0, Y}\)</span> be the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\((A, W)\)</span> wrt dominating
measure <span class="math inline">\(\xi\)</span>. Let <span class="math inline">\(q_{0, A}\)</span> be the conditional density of <span class="math inline">\(A\)</span> given <span class="math inline">\(W\)</span> wrt
dominating measure <span class="math inline">\(\mu\)</span>. Let <span class="math inline">\(q_{0, W}\)</span> be the density of <span class="math inline">\(W\)</span> wrt dominating
measure <span class="math inline">\(\nu\)</span>. Then, for <span class="math inline">\(p_0^O\)</span>, density of <span class="math inline">\(O\)</span> wrt the product measure,
density evaluated on a particular observation <span class="math inline">\(o\)</span>:
<span class="math display">\[\begin{equation*}\label{likelihood_factorization}
  p_0^O(x) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w).
\end{equation*}\]</span></p>
<p>Use a nonparametric structural equation model (NPSEM) to describe
generation of <span class="math inline">\(O\)</span> <span class="citation">Pearl (<a href="#ref-pearl2009causality">2009</a>)</span>, specifically
<span class="math display">\[\begin{align*}
  W &amp;= f_W(U_W) \\ A &amp;= f_A(W, U_A) \\ Y &amp;= f_Y(A, W, U_Y)
\end{align*}\]</span></p>
<p>NPSEM parameterizes <span class="math inline">\(p_0^O\)</span> in terms of the distribution of RVs <span class="math inline">\((O, U)\)</span>
modeled by this system of equations.</p>
<p>Implies a model for the distribution of counterfactual RVs generated by
interventions on the data-generating process.
Notation: let <span class="math inline">\(f_W\)</span>, <span class="math inline">\(f_A\)</span>, <span class="math inline">\(f_Y\)</span> be deterministic functions, and <span class="math inline">\(U_W\)</span>,
<span class="math inline">\(U_A\)</span>, <span class="math inline">\(U_Y\)</span> exogenous RVs.</p>
<p> modify the value <span class="math inline">\(A\)</span> would naturally
assume, <span class="math inline">\(f_A(W, U_A)\)</span>, by drawing from a modified intervention distribution
<span class="math inline">\(G^{\star}(\cdot \mid W)\)</span> so that the new value <span class="math inline">\(A^{\star} \sim G^{\star}(\cdot \mid W)\)</span>.</p>
<p>This generates a counterfactual RV, with distribution <span class="math inline">\(P_{0}^d\)</span>,
<span class="math inline">\(Y_{d(A, W)} := f_Y(d(A,W), W, U_Y) \equiv Y_{G^{\star}} := f_Y(A^{\star}, W, U_Y)\)</span>. We estimate <span class="math inline">\(\psi_{0, d} := \mathbb{E}_{P_0^d}\{Y_{d(A,W)}\}\)</span>, mean of <span class="math inline">\(Y_{d(A, W)}\)</span>.</p>
<p>Starts with Mark and Ivan’s simple stochastic shift.
Extensions to modified treatment policies.
The new value of <span class="math inline">\(A\)</span> may be denoted <span class="math inline">\(A^{\star} \sim G^{\star}(\cdot \mid W)\)</span>,
where <span class="math inline">\(A^{\star} = d(W, U^{\star})\)</span> for a rule <span class="math inline">\(d\)</span> and random error <span class="math inline">\(U^{\star}\)</span>.</p>
<div id="literature-diaz2012population" class="section level4">
<h4><span class="header-section-number">5.5.0.1</span> Literature: <span class="citation">Díaz and van der Laan (<a href="#ref-diaz2012population">2012</a>)</span></h4>
<ul>
<li> Evaluate outcome under an altered
 — e.g.,
<span class="math inline">\(P_{\delta}(g_0)(A = a \mid W) = g_0(a - \delta(W) \mid W)\)</span>.</li>
<li>Identification conditions for a statistical parameter of the
counterfactual outcome <span class="math inline">\(\psi_{0,d}\)</span> under such an intervention.</li>
<li>Show that the causal quantity of interest <span class="math inline">\(\E_0 \{Y_{d(A, W)}\}\)</span> is
identified by a functional of the distribution of <span class="math inline">\(O\)</span>:
<span class="math display">\[\begin{align*}\label{eqn:identification2012}
  \psi_{0,d} = \int_{\mathcal{W}} \int_{\mathcal{A}} &amp; \mathbb{E}_{P_0}
   \{Y \mid A = d(a, w), W = w\} \cdot \\ &amp;q_{0, A}^O(a \mid W = w) \cdot
   q_{0, W}^O(w) d\mu(a)d\nu(w)
\end{align*}\]</span></li>
<li><p>Provides a derivation based on the efficient influence function (EIF) with
respect to the nonparametric model <span class="math inline">\(\mathcal{M}\)</span>.</p></li>
<li>The identification result allows us to write down the causal quantity
of interest in terms of a functional of the observed data.</li>
<li>Key innovation: loosening standard assumptions through a change in
the observed intervention mechanism.</li>
<li>Problem: globally altering an intervention mechanism does not
necessarily respect individual characteristics.</li>
<li>The authors build IPW, A-IPW, and TML estimators, comparing the three
different approaches.</li>
<li><p>IMPORTANT: gives the g-computation formula for identification of this
estimator from the observed data structure.</p></li>
</ul>
</div>
<div id="literature-diaz2018stochastic" class="section level4">
<h4><span class="header-section-number">5.5.0.2</span> Literature: <span class="citation">Díaz and van der Laan (<a href="#ref-diaz2018stochastic">2018</a>)</span></h4>
<ul>
<li>Builds on the original proposal, accommodating shifts <span class="math inline">\(d(A,W)\)</span> proposed
after their earlier work.</li>
<li>To protect against positivity violations, considers a specific shifting
mechanism:
<span class="math display">\[\begin{equation*}\label{shift_intervention}
  d(a, w) =
    \begin{cases}
      a + \delta, &amp; a + \delta &lt; u(w) \\
      a, &amp; \text{otherwise}
    \end{cases}
\end{equation*}\]</span></li>
<li>Proposes an improved “1-TMLE” algorithm, with a single auxiliary
(“clever”) covariate for constructing the TML estimator.</li>
</ul>
<p>Identification: From Causal Inference to Statistics</p>
<ul>
<li>(Assumption 1) <em>Consistency</em>: <span class="math inline">\(Y^{d(a_i, w_i)}_i = Y_i\)</span> in the event
<span class="math inline">\(A_i = d(a_i, w_i)\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span></li>
<li>(Assumption 2) <em>SUTVA</em>: <span class="math inline">\(Y^{d(a_i, w_i)}_i\)</span> does not depend on
<span class="math inline">\(d(a_j, w_j)\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span> and <span class="math inline">\(j \neq i\)</span>, or lack of interference
<span class="citation">(Rubin <a href="#ref-rubin1978bayesian">1978</a>, <a href="#ref-rubin1980randomization">1980</a>)</span></li>
<li>(Assumption 3) <em>Strong ignorability</em>: <span class="math inline">\(A_i \indep Y^{d(a_i, w_i)}_i \mid W_i\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span></li>
<li>(Assumption 4) <em>Positivity (or overlap)</em>: <span class="math inline">\(a_i \in \mathcal{A} \implies d(a_i, w_i) \in \mathcal{A}\)</span> for all <span class="math inline">\(w \in \mathcal{W}\)</span>, where
<span class="math inline">\(\mathcal{A}\)</span> denotes the support of <span class="math inline">\(A \mid W = w_i \quad \forall i = 1, \ldots n\)</span></li>
</ul>
<p>Semiparametric-Efficient Estimation
- Semiparametric-efficient estimation through solving the efficient influence
function estimating equation wrt <span class="math inline">\(\M\)</span>.
- Statistical target parameter:
<span class="math inline">\(\Psi(P_0) = \mathbb{E}_{P_0}{\overline{Q}(d(A, W), W)}\)</span>
- For which the efficient influence function (EIF) is
<span class="math display">\[\begin{equation*}
    D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
    \overline{Q}(d(a, w), w) - \Psi(P_0)
  \end{equation*}\]</span>
- The auxiliary covariate <span class="math inline">\(H(a,w)\)</span> may be expressed
<span class="math display">\[\begin{equation*}
    H(a,w) = \mathbb{I}(a + \delta &lt; u(w)) \frac{g_0(a - \delta \mid w)}
    {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w))
  \end{equation*}\]</span>
- The auxiliary covariate simplifies when the treatment is in the limits
(conditional on <span class="math inline">\(W\)</span>) — i.e., for <span class="math inline">\(A_i \in (u(w) - \delta, u(w))\)</span>, then
we have <span class="math inline">\(H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1\)</span>.
- Need to explicitly remind the audience what <span class="math inline">\(u(w)\)</span> is again. It has only
appeared once at this point, and only been mentioned in passing.</p>
<ul>
<li><strong>Asymptotic linearity:</strong>
<span class="math display">\[\begin{equation*}
  \Psi(P_n^{\star}) - \Psi(P_0) = \frac{1}{n} \sum_{i = 1}^{n} D(P_0)(X_i) +
  o_P\left(\frac{1}{\sqrt{n}}\right)
\end{equation*}\]</span></li>
<li>Gaussian limiting distribution:
<span class="math display">\[\begin{equation*}
  \sqrt{n}(\Psi(P_n^{\star}) - \Psi(P_0)) \to N(0, Var(D(P_0)(O)))
\end{equation*}\]</span></li>
<li>Statistical inference:
<span class="math display">\[\begin{equation*}
  \text{Wald-type CI}: \Psi(P_n^{\star}) \pm z_{\alpha} \cdot
  \frac{\sigma_n}{\sqrt{n}},
\end{equation*}\]</span>
where <span class="math inline">\(\sigma_n^2\)</span> is computed directly via
<span class="math inline">\(\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\cdot)(O_i)\)</span>.</li>
</ul>
<p>Under the additional condition that the remainder term <span class="math inline">\(R(\hat{P}^*, P_0)\)</span>
decays as <span class="math inline">\(o_P \left( \frac{1}{\sqrt{n}} \right),\)</span> we have that
<span class="math inline">\(\Psi_n - \Psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}} \right),\)</span> which, by a central limit theorem,
establishes a Gaussian limiting distribution for the estimator, with variance
<span class="math inline">\(V(D(P_0))\)</span>, the variance of the efficient influence function
when <span class="math inline">\(\Psi\)</span> admits an asymptotically linear representation.</p>
<p>The above implies that <span class="math inline">\(\Psi_n\)</span> is a <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator of <span class="math inline">\(\Psi\)</span>,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals, where
<span class="math inline">\(\sigma_n^2\)</span> is an estimator of <span class="math inline">\(V(D(P_0))\)</span>. The estimator <span class="math inline">\(\sigma_n^2\)</span>
may be obtained using the bootstrap or computed directly via
<span class="math inline">\(\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)\)</span></p>
<p>We obtain semiparametric-efficient estimation and robust inference in the
nonparametric model <span class="math inline">\(\M\)</span> by solving the efficient influence function.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(D(\bar{Q}_n^*, g_n)\)</span> converges to <span class="math inline">\(D(P_0)\)</span> in <span class="math inline">\(L_2(P_0)\)</span> norm.</li>
<li>The size of the class of functions <span class="math inline">\(\bar{Q}_n^*\)</span> and <span class="math inline">\(g_n\)</span> is bounded
(technically, <span class="math inline">\(\exists \mathcal{F}\)</span> st
<span class="math inline">\(D(\bar{Q}_n^*, g_n) \in \mathcal{F}\)</span> whp, where <span class="math inline">\(\mathcal{F}\)</span> is a
Donsker class)</li>
</ol>
<ul>
<li>Construct initial estimators <span class="math inline">\(g_n\)</span> of <span class="math inline">\(g_0(A, W)\)</span> and <span class="math inline">\(Q_n\)</span> of
<span class="math inline">\(\overline{Q}_0(A, W)\)</span>, perhaps using data-adaptive regression techniques.</li>
<li>For each observation <span class="math inline">\(i\)</span>, compute an estimate <span class="math inline">\(H_n(a_i, w_i)\)</span> of the
auxiliary covariate <span class="math inline">\(H(a_i,w_i)\)</span>.</li>
<li>Estimate the parameter <span class="math inline">\(\epsilon\)</span> in the logistic regression model
<span class="math display">\[ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
\text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),\]</span>
or an alternative regression model incorporating weights.</li>
<li><p>Compute TML estimator <span class="math inline">\(\Psi_n\)</span> of the target parameter, defining update
<span class="math inline">\(\overline{Q}_n^{\star}\)</span> of the initial estimate
<span class="math inline">\(\overline{Q}_{n, \epsilon_n}\)</span>:
<span class="math display">\[\begin{equation*}\label{tmle}
\Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
\overline{Q}_n^{\star}(d(A_i, W_i), W_i).
\end{equation*}\]</span></p></li>
<li>We recommend using nonparametric methods for the initial estimators,
as consistent estimation is necessary for efficiency of the estimator
<span class="math inline">\(\Psi_n\)</span>.</li>
<li><p>Intuition for the submodel fluctuation?</p></li>
</ul>
</div>
<div id="literature-haneuse2013estimation" class="section level4">
<h4><span class="header-section-number">5.5.0.3</span> Literature: <span class="citation">Haneuse and Rotnitzky (<a href="#ref-haneuse2013estimation">2013</a>)</span></h4>
<ul>
<li> Characterization of stochastic interventions as
 (MTPs).</li>
<li>Assumption of  allows for the
intervention distribution of any MTP to be recovered:
<span class="math display">\[\begin{equation*}
  g_{0, \delta}(a \mid w) = \sum_{j = 1}^{J(w)} I_{\delta, j} \{h_j(a, w),
  w\} g_0\{h_j(a, w) \mid w\} h^{\prime}_j(a,w)
\end{equation*}\]</span></li>
<li>Such intervention policies account for the natural value of the
intervention <span class="math inline">\(A\)</span> directly yet are interpretable as the imposition of an
altered intervention mechanism.</li>
<li>Identification conditions for assessing the parameter of interest under
such interventions appear technically complex (at first).</li>
<li>Shifts of the form <span class="math inline">\(d(A,W)\)</span> are considerably more interesting since
these are realistic intervention policies.</li>
<li>Example: consider an individual with an extremely high immune response
but whose baseline covariates <span class="math inline">\(W\)</span> suggest we shift the response still
higher. Such a shift may not be biologically plausible (impossible, even)
but we cannot account for this if the shift is only a function of <span class="math inline">\(W\)</span>.</li>
<li>The authors build IPW, outcome regression, and non-iterative doubly
robust estimators, as well as an approach based on MSMs.</li>
<li>Piecewise smooth invertibility: This assumption ensures that we can
use the change of variable formula when computing integrals over <span class="math inline">\(A\)</span> and
it is useful to study the estimators that we propose in this paper.</li>
</ul>
</div>
<div id="literature-young2014identification" class="section level4">
<h4><span class="header-section-number">5.5.0.4</span> Literature: <span class="citation">Young, Hernán, and Robins (<a href="#ref-young2014identification">2014</a>)</span></h4>
<ul>
<li>Establishes equivalence between g-formula when proposed intervention
depends on natural value and when it does not.</li>
<li>This equivalence leads to a sufficient positivity condition for
estimating the counterfactual mean under MTPs via the same statistical
functional studied in .</li>
<li>Extends earlier identification results, providing a way to use the same
statistical functional to assess <span class="math inline">\(\mathbb{E} Y_{d(A,W)}\)</span> or
<span class="math inline">\(\mathbb{E} Y_{d(W)}\)</span>.</li>
<li>The authors also consider limits on implementing shifts <span class="math inline">\(d(A,W)\)</span>, and
address working in a longitudinal setting.</li>
</ul>
</div>
</div>
<div id="interpreting-the-causal-effect-of-a-stochastic-intervention" class="section level2">
<h2><span class="header-section-number">5.6</span> Interpreting the Causal Effect of a Stochastic Intervention</h2>
<p>TODO</p>
</div>
<div id="evaluating-the-causal-effect-of-a-stochastic-intervention" class="section level2">
<h2><span class="header-section-number">5.7</span> Evaluating the Causal Effect of a Stochastic Intervention</h2>
<p>To start, let us load the packages we will use and set a seed for simulation:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(data.table)
<span class="kw">library</span>(condensier)
<span class="kw">library</span>(sl3)
<span class="kw">library</span>(tmle3)
<span class="kw">library</span>(tmle3shift)
<span class="kw">set.seed</span>(<span class="dv">429153</span>)</code></pre>
<p>We need to estimate two components of the likelihood in order to…</p>
<p>The first of these is the outcome regression, <span class="math inline">\(\hat{Q}_n\)</span>, which is a simple
regression of the form <span class="math inline">\(\mathbb{E}[Y \mid A, W]\)</span>. An estimate for such a
quantity may be constructed using the Super Learner algorithm:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># learners used for conditional expectation regression</span>
lrn_mean &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()
lrn_fglm &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()
lrn_xgb &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds=</span><span class="dv">200</span>)
lrn_hal &lt;-<span class="st"> </span>Lrnr_hal9001<span class="op">$</span><span class="kw">new</span>()
sl_lrn &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_mean, lrn_fglm), <span class="co">#, lrn_xgb, lrn_hal),</span>
  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()
)</code></pre>
<p>The second of these is an estimate of the treatment mechanism, <span class="math inline">\(\hat{g}_n\)</span>,
i.e., the <em>propensity score</em>. In the case of a continuous intervention node
<span class="math inline">\(A\)</span>, such a quantity takes the form <span class="math inline">\(p(A \mid W)\)</span>, which is a conditional
density. Generally speaking, conditional density estimation is a challenging
problem that has received much attention in the literature (see, for example,
…). To perform conditional density estimation, we focus on approach…</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># learners used for conditional density regression</span>
lrn_mean_dens &lt;-<span class="st"> </span>Lrnr_condensier<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">nbins =</span> <span class="dv">20</span>, <span class="dt">bin_estimator =</span> lrn_mean,
  <span class="dt">bin_method =</span> <span class="st">&quot;dhist&quot;</span>
)
lrn_fglm_dens &lt;-<span class="st"> </span>Lrnr_condensier<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">nbins =</span> <span class="dv">10</span>, <span class="dt">bin_estimator =</span> lrn_fglm,
  <span class="dt">bin_method =</span> <span class="st">&quot;dhist&quot;</span>
)
lrn_xgb_dens &lt;-<span class="st"> </span>Lrnr_condensier<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">nbins =</span> <span class="dv">5</span>, <span class="dt">bin_estimator =</span> lrn_xgb,
  <span class="dt">bin_method =</span> <span class="st">&quot;dhist&quot;</span>
)
sl_lrn_dens &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_mean_dens, lrn_fglm_dens, lrn_xgb_dens),
  <span class="dt">metalearner =</span> Lrnr_solnp_density<span class="op">$</span><span class="kw">new</span>()
)

<span class="co"># specify outcome and treatment regressions and create learner list</span>
Q_learner &lt;-<span class="st"> </span>sl_lrn
g_learner &lt;-<span class="st"> </span>sl_lrn_dens
learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner)</code></pre>
<p>The <code>learner_list</code> object above specifies the role that each of the ensemble
learners we have generated is to play in computing initial estimators to be
used in building a TMLE for the parameter of interest here. In particular, it
makes explicit the fact that our <code>Q_learner</code> is used in fitting the outcome
regression while our <code>g_learner</code> is used in estimating the treatment mechanism.</p>
<div id="simulate-data" class="section level3">
<h3><span class="header-section-number">5.7.1</span> Simulate Data</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate simple data for tmle-shift sketch</span>
n_obs &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># number of observations</span>
tx_mult &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># multiplier for the effect of W = 1 on the treatment</span>

## baseline covariates -- simple, binary
W &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">2</span>, <span class="kw">rbinom</span>(n_obs, <span class="dv">1</span>, <span class="fl">0.5</span>))

## create treatment based on baseline W
A &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_obs, <span class="dt">mean =</span> tx_mult <span class="op">*</span><span class="st"> </span>W, <span class="dt">sd =</span> <span class="dv">1</span>)

## create outcome as a linear function of A, W + white noise
Y &lt;-<span class="st"> </span><span class="kw">rbinom</span>(n_obs, <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">plogis</span>(A <span class="op">+</span><span class="st"> </span>W))

<span class="co"># organize data and nodes for tmle3</span>
data &lt;-<span class="st"> </span><span class="kw">data.table</span>(W, A, Y)
<span class="kw">setnames</span>(data, <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;Y&quot;</span>))
node_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>), <span class="dt">A =</span> <span class="st">&quot;A&quot;</span>, <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span>)
<span class="kw">head</span>(data)</code></pre>
<pre><code>   W1 W2          A Y
1:  1  1  3.5806529 1
2:  1  0  3.2071846 1
3:  1  1  1.0358382 1
4:  0  0 -0.6578495 1
5:  1  1  3.0199033 1
6:  1  1  2.7803127 1</code></pre>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. To formally
express this fact using the <code>tlverse</code> grammar introduced by the <code>tmle3</code> package,
we create a single data object and specify the functional relationships between
the nodes in the <em>directed acyclic graph</em> (DAG) via <em>nonparametric structural
equation models</em> (NPSEMs), reflected in the node list that we set up:</p>
<p>We now have an observed data structure (<code>data</code>) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.</p>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest (called a <code>tmle3_Spec</code> in the <code>tlverse</code> nomenclature) simply by calling
<code>tmle_shift</code>. We specify the argument <code>shift_val = 0.5</code> when initializing the
<code>tmle3_Spec</code> object to communicate that we’re interested in a shift of <span class="math inline">\(0.5\)</span> on
the scale of the treatment <span class="math inline">\(A\)</span> – that is, we specify <span class="math inline">\(\delta = 0.5\)</span> (note that
this is an arbitrarily chosen value for this example).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle_shift</span>(<span class="dt">shift_val =</span> <span class="fl">0.5</span>,
                        <span class="dt">shift_fxn =</span> shift_additive_bounded,
                        <span class="dt">shift_fxn_inv =</span> shift_additive_bounded_inv)</code></pre>
<p>As seen above, the <code>tmle_shift</code> specification object (like all <code>tmle3_Spec</code>
objects) does <em>not</em> store the data for our specific analysis of interest. Later,
we’ll see that passing a data object directly to the <code>tmle3</code> wrapper function,
alongside the instantiated <code>tmle_spec</code>, will serve to construct a <code>tmle3_Task</code>
object internally (see the <code>tmle3</code> documentation for details).</p>
</div>
<div id="targeted-estimation-of-stochastic-interventions-effects" class="section level3">
<h3><span class="header-section-number">5.7.2</span> Targeted Estimation of Stochastic Interventions Effects</h3>
<pre class="sourceCode r"><code class="sourceCode r">tmle_fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</code></pre>
<pre><code>
Iter: 1 fn: 1845.0103    Pars:  0.9513029795 0.0486969185 0.0000001021
Iter: 2 fn: 1845.0103    Pars:  0.95130300211 0.04869694590 0.00000005199
solnp--&gt; Completed in 2 iterations</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tmle_fit</code></pre>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est         se   lower     upper
1:  TSM E[Y_{A=NULL}] 0.7977323 0.7948962 0.01189624 0.77158 0.8182124
   psi_transformed lower_transformed upper_transformed
1:       0.7948962           0.77158         0.8182124</code></pre>
<p>The <code>print</code> method of the resultant <code>tmle_fit</code> object conveniently displays the
results from computing our TML estimator.</p>
</div>
<div id="statistical-inference-for-targeted-maximum-likelihood-estimates" class="section level3">
<h3><span class="header-section-number">5.7.3</span> Statistical Inference for Targeted Maximum Likelihood Estimates</h3>
<p>Recall that the asymptotic distribution of TML estimators has been studied
thoroughly:
<span class="math display">\[\psi_n - \psi_0 = (P_n - P_0) \cdot D(\bar{Q}_n^*, g_n) + R(\hat{P}^*, P_0),\]</span>
which, provided the following two conditions:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(D(\bar{Q}_n^*, g_n)\)</span> converges to <span class="math inline">\(D(P_0)\)</span> in <span class="math inline">\(L_2(P_0)\)</span> norm, and</li>
<li>the size of the class of functions considered for estimation of <span class="math inline">\(\bar{Q}_n^*\)</span>
and <span class="math inline">\(g_n\)</span> is bounded (technically, <span class="math inline">\(\exists \mathcal{F}\)</span> st
<span class="math inline">\(D(\bar{Q}_n^*, g_n) \in \mathcal{F}\)</span> <em><strong>whp</strong></em>, where <span class="math inline">\(\mathcal{F}\)</span> is a
Donsker class),
readily admits the conclusion that
<span class="math inline">\(\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + R(\hat{P}^*, P_0)\)</span>.</li>
</ol>
<p>Under the additional condition that the remainder term <span class="math inline">\(R(\hat{P}^*, P_0)\)</span>
decays as <span class="math inline">\(o_P \left( \frac{1}{\sqrt{n}} \right),\)</span> we have that
<span class="math display">\[\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}}
 \right),\]</span>
which, by a central limit theorem, establishes a Gaussian limiting distribution
for the estimator:</p>
<p><span class="math display">\[\sqrt{n}(\psi_n - \psi) \to N(0, V(D(P_0))),\]</span>
where <span class="math inline">\(V(D(P_0))\)</span> is the variance of the efficient influence curve (canonical
gradient) when <span class="math inline">\(\psi\)</span> admits an asymptotically linear representation.</p>
<p>The above implies that <span class="math inline">\(\psi_n\)</span> is a <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator of <span class="math inline">\(\psi\)</span>,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals in a
straightforward manner:</p>
<p><span class="math display">\[\psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}},\]</span>
where <span class="math inline">\(\sigma_n^2\)</span> is an estimator of <span class="math inline">\(V(D(P_0))\)</span>. The estimator <span class="math inline">\(\sigma_n^2\)</span>
may be obtained using the bootstrap or computed directly via the following</p>
<p><span class="math display">\[\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)\]</span></p>
<p>Having now re-examined these facts, let’s simply examine the results of
computing our TML estimator:</p>
<hr />
</div>
</div>
<div id="extensions-variable-importance-analysis-with-stochastic-interventions" class="section level2">
<h2><span class="header-section-number">5.8</span> Extensions: Variable Importance Analysis with Stochastic Interventions</h2>
<div id="defining-a-grid-of-counterfactual-interventions" class="section level3">
<h3><span class="header-section-number">5.8.1</span> Defining a grid of counterfactual interventions</h3>
<p>In order to specify a <em>grid</em> of shifts <span class="math inline">\(\delta\)</span> to be used in defining a set of
stochastic intervention policies in an <em>a priori</em> manner, let us consider an
arbitrary scalar <span class="math inline">\(\delta\)</span> that defines a counterfactual outcome <span class="math inline">\(\psi_n = Q_n(d(A, W), W)\)</span>, where, for simplicity, let <span class="math inline">\(d(A, W) = A + \delta\)</span>. A
simplified expression of the auxiliary covariate for the TMLE of <span class="math inline">\(\psi\)</span> is
<span class="math inline">\(H_n = \frac{g^*(a \mid w)}{g(a \mid w)}\)</span>, where <span class="math inline">\(g^*(a \mid w)\)</span> defines the
treatment mechanism with the stochastic intervention implemented. Then, to
ascertain whether a given choice of the shift <span class="math inline">\(\delta\)</span> is admissable (in the
sense of avoiding violations of the positivity assumption), let there be a bound
<span class="math inline">\(C(\delta) = \frac{g^*(a \mid w)}{g(a \mid w)} &lt; M\)</span>, where <span class="math inline">\(g^*(a \mid w)\)</span> is a
function of <span class="math inline">\(\delta\)</span> in part, and <span class="math inline">\(M\)</span> is a potentially user-specified upper
bound of <span class="math inline">\(C(\delta)\)</span>. Then, <span class="math inline">\(C(\delta)\)</span> is a measure of the influence of a given
observation (under a bound of the conditional densities), which provides a way
to limit the maximum influence of a given observation through a choice of the
shift <span class="math inline">\(\delta\)</span>.</p>
<p>We formalize and extend the procedure to determine an acceptable set of values
for the shift <span class="math inline">\(\delta\)</span> in the sequel. Specifically, let there be a shift <span class="math inline">\(d(A, W) = A + \delta(A, W)\)</span>, where the shift <span class="math inline">\(\delta(A, W)\)</span> is defined as
<span class="math display">\[\begin{equation}
  \delta(a, w) =
    \begin{cases}
      \delta, &amp; \delta_{\text{min}}(a,w) \leq \delta \leq
        \delta_{\text{max}}(a,w) \\
      \delta_{\text{max}}(a,w), &amp; \delta \geq \delta_{\text{max}}(a,w) \\
      \delta_{\text{min}}(a,w), &amp; \delta \leq \delta_{\text{min}}(a,w) \\
    \end{cases},
\end{equation}\]</span>
where <span class="math display">\[\delta_{\text{max}}(a, w) = \text{argmax}_{\left\{\delta \geq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}\]</span> and
<span class="math display">\[\delta_{\text{min}}(a, w) = \text{argmin}_{\left\{\delta \leq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}.\]</span></p>
<p>The above provides a strategy for implementing a shift at the level of a given
observation <span class="math inline">\((a_i, w_i)\)</span>, thereby allowing for all observations to be shifted to
an appropriate value – whether <span class="math inline">\(\delta_{\text{min}}\)</span>, <span class="math inline">\(\delta\)</span>, or
<span class="math inline">\(\delta_{\text{max}}\)</span>.</p>
</div>
<div id="initializing-vimshift-through-its-tmle3_spec" class="section level3">
<h3><span class="header-section-number">5.8.2</span> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></h3>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest (called a <code>tmle3_Spec</code> in the <code>tlverse</code> nomenclature) simply by calling
<code>tmle_shift</code>. We specify the argument <code>shift_grid = seq(-1, 1, by = 1)</code>
when initializing the <code>tmle3_Spec</code> object to communicate that we’re interested
in assessing the mean counterfactual outcome over a grid of shifts -1, 0, 1 on the scale of the treatment <span class="math inline">\(A\)</span> (note that the numerical
choice of shift is an arbitrarily chosen set of values for this example).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># what&#39;s the grid of shifts we wish to consider?</span>
delta_grid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)

<span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle_vimshift_delta</span>(<span class="dt">shift_grid =</span> delta_grid,
                                 <span class="dt">max_shifted_ratio =</span> <span class="dv">2</span>)</code></pre>
<p>As seen above, the <code>tmle_vimshift</code> specification object (like all <code>tmle3_Spec</code>
objects) does <em>not</em> store the data for our specific analysis of interest. Later,
we’ll see that passing a data object directly to the <code>tmle3</code> wrapper function,
alongside the instantiated <code>tmle_spec</code>, will serve to construct a <code>tmle3_Task</code>
object internally (see the <code>tmle3</code> documentation for details).</p>
</div>
<div id="targeted-estimation-of-stochastic-interventions-effects-1" class="section level3">
<h3><span class="header-section-number">5.8.3</span> Targeted Estimation of Stochastic Interventions Effects</h3>
<p>One may walk through the step-by-step procedure for fitting the TML estimator
of the mean counterfactual outcome under each shift in the grid, using the
machinery exposed by the <a href="https://tmle3.tlverse.org/"><code>tmle3</code> R package</a>.</p>
<p>One may invoke the <code>tmle3</code> wrapper function (a user-facing convenience utility)
to fit the series of TML estimators (one for each parameter defined by the grid
delta) in a single function call:</p>
<pre class="sourceCode r"><code class="sourceCode r">tmle_fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</code></pre>
<pre><code>
Iter: 1 fn: 1844.2284    Pars:  0.96656215268 0.03343778776 0.00000005682
Iter: 2 fn: 1844.2284    Pars:  0.966562195572 0.033437800382 0.000000004046
solnp--&gt; Completed in 2 iterations</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tmle_fit</code></pre>
<pre><code>A tmle3_Fit that took 1 step(s)
         type          param  init_est  tmle_est          se     lower
1:        TSM  E[Y_{A=NULL}] 0.6128645 0.6127271 0.015541581 0.5822662
2:        TSM  E[Y_{A=NULL}] 0.7389799 0.7390000 0.013895038 0.7117662
3:        TSM  E[Y_{A=NULL}] 0.8489662 0.8495258 0.009815749 0.8302873
4: MSM_linear MSM(intercept) 0.7333998 0.7335228 0.012680492 0.7086695
5: MSM_linear     MSM(slope) 0.1169330 0.1173696 0.004617009 0.1083205
       upper psi_transformed lower_transformed upper_transformed
1: 0.6431881       0.6127271         0.5822662         0.6431881
2: 0.7662338       0.7390000         0.7117662         0.7662338
3: 0.8687643       0.8495258         0.8302873         0.8687643
4: 0.7583761       0.7335228         0.7086695         0.7583761
5: 0.1264188       0.1173696         0.1083205         0.1264188</code></pre>
<p><em>Remark</em>: The <code>print</code> method of the resultant <code>tmle_fit</code> object conveniently
displays the results from computing our TML estimator.</p>
</div>
<div id="inference-with-marginal-structural-models" class="section level3">
<h3><span class="header-section-number">5.8.4</span> Inference with Marginal Structural Models</h3>
<p>In the directly preceding section, we consider estimating the mean
counterfactual outcome <span class="math inline">\(\psi_n\)</span> under several values of the intervention
<span class="math inline">\(\delta\)</span>, taken from the aforementioned <span class="math inline">\(\delta\)</span>-grid. We now turn our attention
to an approach for obtaining inference on a single summary measure of these
estimated quantities. In particular, we propose summarizing the estimates
<span class="math inline">\(\psi_n\)</span> through a marginal structural model (MSM), obtaining inference by way
of a hypothesis test on a parameter of this working MSM. For a data structure
<span class="math inline">\(O = (W, A, Y)\)</span>, let <span class="math inline">\(\psi_{\delta}(P_0)\)</span> be the mean outcome under a shift
<span class="math inline">\(\delta\)</span> of the treatment, so that we have <span class="math inline">\(\vec{\psi}_{\delta} = (\psi_{\delta}: \delta)\)</span> with corresponding estimators <span class="math inline">\(\vec{\psi}_{n, \delta} = (\psi_{n, \delta}: \delta)\)</span>. Further, let <span class="math inline">\(\beta(\vec{\psi}_{\delta}) = \phi((\psi_{\delta}: \delta))\)</span>.</p>
<p>For a given MSM <span class="math inline">\(m_{\beta}(\delta)\)</span>, we have that
<span class="math display">\[\beta_0 = \text{argmin}_{\beta} \sum_{\delta}(\psi_{\delta}(P_0) -
m_{\beta}(\delta))^2 h(\delta),\]</span>
which is the solution to
<span class="math display">\[u(\beta, (\psi_{\delta}: \delta)) = \sum_{\delta}h(\delta)
\left(\psi_{\delta}(P_0) - m_{\beta}(\delta) \right) \frac{d}{d\beta}
m_{\beta}(\delta) = 0.\]</span>
This then leads to the following expansion
<span class="math display">\[\beta(\vec{\psi}_n) - \beta(\vec{\psi}_0) \approx -\frac{d}{d\beta} u(\beta_0,
\vec{\psi}_0)^{-1} \frac{d}{d\psi} u(\beta_0, \psi_0)(\vec{\psi}_n -
\vec{\psi}_0),\]</span>
where we have
<span class="math display">\[\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta)
-\sum_{\delta} h(\delta) m_{\beta}(\delta) \frac{d^2}{d\beta^2}
m_{\beta}(\delta),\]</span>
which, in the case of an MSM that is a linear model (since
<span class="math inline">\(\frac{d^2}{d\beta^2} m_{\beta}(\delta) = 0\)</span>), reduces simply to
<span class="math display">\[\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta),\]</span>
and
<span class="math display">\[\frac{d}{d\psi}u(\beta, \psi)(\psi_n - \psi_0) = \sum_{\delta} h(\delta)
\frac{d}{d\beta} m_{\beta}(\delta) (\psi_n - \psi_0)(\delta),\]</span>
which we may write in terms of the efficient influence function (EIF) of <span class="math inline">\(\psi\)</span>
by using the first order approximation <span class="math inline">\((\psi_n - \psi_0)(\delta) = \frac{1}{n}\sum_{i = 1}^n \text{EIF}_{\psi_{\delta}}(O_i)\)</span>,
where <span class="math inline">\(\text{EIF}_{\psi_{\delta}}\)</span> is the efficient influence function (EIF) of
<span class="math inline">\(\vec{\psi}\)</span>.</p>
<p>Now, say, <span class="math inline">\(\vec{\psi} = (\psi(\delta): \delta)\)</span> is d-dimensional, then we may
write the efficient influence function of the MSM parameter <span class="math inline">\(\beta\)</span> (assuming a
linear MSM) as follows
<span class="math display">\[\text{EIF}_{\beta}(O) = \left(\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta) \frac{d}{d\beta} m_{\beta}(\delta)^t \right)^{-1} \cdot
\sum_{\delta} h(\delta) \frac{d}{d\beta} m_{\beta}(\delta)
\text{EIF}_{\psi_{\delta}}(O),\]</span> where the first term is of dimension
<span class="math inline">\(d \times d\)</span> and the second term is of dimension <span class="math inline">\(d \times 1\)</span>.</p>
<p>In an effort to generalize still further, consider the case where
<span class="math inline">\(\psi_{\delta}(P_0) \in (0, 1)\)</span> – that is, <span class="math inline">\(\psi_{\delta}(P_0)\)</span> corresponds
to the probability of some event of interest. In such a case, it would be more
natural to consider a logistic MSM
<span class="math display">\[m_{\beta}(\delta) = \frac{1}{1 + \exp(-f_{\beta}(\delta))},\]</span>
where <span class="math inline">\(f_{\beta}\)</span> is taken to be linear in <span class="math inline">\(\beta\)</span> (e.g.,
<span class="math inline">\(f_{\beta} = \beta_0 + \beta_1 \delta + \ldots\)</span>). In such a case, we have the
parameter of interest
<span class="math display">\[\beta_0 = \text{argmax}_{\beta} \sum_{\delta} \left(\psi_{\delta}(P_0)
\text{log} m_{\beta}(\delta) + (1 - \psi_{\delta}(P_0))\log(1 -
m_{\beta}(\delta))\right)h(\delta),\]</span>
where <span class="math inline">\(\beta_0\)</span> solves the following
<span class="math display">\[
\sum_{\delta} h(\delta) \frac{d}{d\beta} f_{\beta}(\delta) (\psi_{\delta}(P_0)
- m_{\beta}(\delta)) = 0.\]</span></p>
<p>Inference from a working MSM is rather straightforward. To wit, the limiting
distribution for <span class="math inline">\(m_{\beta}(\delta)\)</span> may be expressed
<span class="math display">\[\sqrt{n}(\beta_n - \beta_0) \to N(0, \Sigma),\]</span>
where <span class="math inline">\(\Sigma\)</span> is the empirical covariance matrix of <span class="math inline">\(\text{EIF}_{\beta}(O)\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r">tmle_fit<span class="op">$</span>summary[<span class="dv">4</span><span class="op">:</span><span class="dv">5</span>, ]</code></pre>
<pre><code>         type          param  init_est  tmle_est          se     lower
1: MSM_linear MSM(intercept) 0.7333998 0.7335228 0.012680492 0.7086695
2: MSM_linear     MSM(slope) 0.1169330 0.1173696 0.004617009 0.1083205
       upper psi_transformed lower_transformed upper_transformed
1: 0.7583761       0.7335228         0.7086695         0.7583761
2: 0.1264188       0.1173696         0.1083205         0.1264188</code></pre>
<div id="directly-targeting-the-msm-parameter-beta" class="section level4">
<h4><span class="header-section-number">5.8.4.1</span> Directly Targeting the MSM Parameter <span class="math inline">\(\beta\)</span></h4>
<p>Note that in the above, a working MSM is fit to the individual TML estimates of
the mean counterfactual outcome under a given value of the shift <span class="math inline">\(\delta\)</span> in the
supplied grid. The parameter of interest <span class="math inline">\(\beta\)</span> of the MSM is asymptotically
linear (and, in fact, a TML estimator) as a consequence of its construction from
individual TML estimators. In smaller samples, it may be prudent to perform a
TML estimation procedure that targets the parameter <span class="math inline">\(\beta\)</span> directly, as opposed
to constructing it from several independently targeted TML estimates. An
approach for constructing such an estimator is proposed in the sequel.</p>
<p>Suppose a simple working MSM <span class="math inline">\(\mathbb{E}Y_{g^0_{\delta}} = \beta_0 + \beta_1 \delta\)</span>, then a TML estimator targeting <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> may be
constructed as
<span class="math display">\[\overline{Q}_{n, \epsilon}(A,W) = \overline{Q}_n(A,W) + \epsilon (H_1(g),
H_2(g),\]</span> for all <span class="math inline">\(\delta\)</span>, where <span class="math inline">\(H_1(g)\)</span> is the auxiliary covariate for
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(H_2(g)\)</span> is the auxiliary covariate for <span class="math inline">\(\beta_1\)</span>.</p>
<p>To construct a targeted maximum likelihood estimator that directly targets the
parameters of the working marginal structural model, we may use the
<code>tmle_vimshift_msm</code> Spec (instead of the <code>tmle_vimshift_delta</code> Spec that
appears above):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_msm_spec &lt;-<span class="st"> </span><span class="kw">tmle_vimshift_msm</span>(<span class="dt">shift_grid =</span> delta_grid,
                                   <span class="dt">max_shifted_ratio =</span> <span class="dv">2</span>)

<span class="co"># fit the TML estimator and examine the results</span>
tmle_msm_fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_msm_spec, data, node_list, learner_list)</code></pre>
<pre><code>
Iter: 1 fn: 1838.4669    Pars:  0.96061554500 0.03938437967 0.00000007423
Iter: 2 fn: 1838.4668    Pars:  0.960615569450 0.039384428822 0.000000001728
solnp--&gt; Completed in 2 iterations</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tmle_msm_fit</code></pre>
<pre><code>A tmle3_Fit that took 100 step(s)
         type          param  init_est  tmle_est          se     lower
1: MSM_linear MSM(intercept) 0.7331760 0.7331760 0.012769216 0.7081488
2: MSM_linear     MSM(slope) 0.1167797 0.1167797 0.004687733 0.1075919
       upper psi_transformed lower_transformed upper_transformed
1: 0.7582032       0.7331760         0.7081488         0.7582032
2: 0.1259674       0.1167797         0.1075919         0.1259674</code></pre>
<hr />
</div>
</div>
</div>
<div id="exercises-1" class="section level2">
<h2><span class="header-section-number">5.9</span> Exercises</h2>
<div id="basicsreview-1" class="section level3">
<h3><span class="header-section-number">5.9.1</span> Basics/Review</h3>
<ol style="list-style-type: decimal">
<li><p>TODO</p></li>
<li><p>Set the <code>sl3</code> library of algorithms for the Super Learner</p></li>
<li><p>TODO</p></li>
<li><p>Describe two (equivalent) ways in which the causal effects of stochastic
interventions may be interpreted.</p></li>
</ol>
</div>
<div id="using-the-ideas-1" class="section level3">
<h3><span class="header-section-number">5.9.2</span> Using the Ideas</h3>
<ol style="list-style-type: decimal">
<li><p>Choose a different variable of interest (e.g., TBD) and repeat the initial
analysis we performed. That is, estimate the counterfactual mean under a
shift of the new variable, after standardizing the chosen variable to have
zero mean and unit variance.</p></li>
<li><p>TODO</p></li>
<li><p>TODO</p></li>
<li><p>What advantages, if any, are there to targeted directly the parameters of a
marginal structural model?</p></li>
</ol>
</div>
<div id="advanced-1" class="section level3">
<h3><span class="header-section-number">5.9.3</span> Advanced</h3>
<ol style="list-style-type: decimal">
<li><p>How does the marginal structural model we used to summarize…</p></li>
<li><p>TODO</p></li>
</ol>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-diaz2012population">
<p>Díaz, Iván, and Mark J van der Laan. 2012. “Population Intervention Causal Effects Based on Stochastic Interventions.” <em>Biometrics</em> 68 (2). Wiley Online Library: 541–49.</p>
</div>
<div id="ref-diaz2018stochastic">
<p>Díaz, Iván, and Mark J van der Laan. 2018. “Stochastic Treatment Regimes.” In <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>, 167–80. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-haneuse2013estimation">
<p>Haneuse, Sebastian, and Andrea Rotnitzky. 2013. “Estimation of the Effect of Interventions That Modify the Received Treatment.” <em>Statistics in Medicine</em> 32 (30). Wiley Online Library: 5260–77.</p>
</div>
<div id="ref-pearl2009causality">
<p>Pearl, Judea. 2009. <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press.</p>
</div>
<div id="ref-rubin1978bayesian">
<p>Rubin, Donald B. 1978. “Bayesian Inference for Causal Effects: The Role of Randomization.” <em>The Annals of Statistics</em>. JSTOR, 34–58.</p>
</div>
<div id="ref-rubin1980randomization">
<p>Rubin, Donald B. 1980. “Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment.” <em>Journal of the American Statistical Association</em> 75 (371). JSTOR: 591–93.</p>
</div>
<div id="ref-vdl2011targeted">
<p>van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-vdl2018targeted">
<p>van der Laan, Mark J, and Sherri Rose. 2018. <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-young2014identification">
<p>Young, Jessica G, Miguel A Hernán, and James M Robins. 2014. “Identification, Estimation and Approximation of Risk Under Interventions That Depend on the Natural Value of Treatment Using Observational Data.” <em>Epidemiologic Methods</em> 3 (1). De Gruyter: 1–19.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimal-individualized-treatment-regimes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/acic2019-workshop/edit/master/05-tmle3shift.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["handbook.pdf", "handbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
