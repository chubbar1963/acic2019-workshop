# `sl3`: Modern Super Learning with Pipelines
<!--
NOTE: YOU CAN'T USE THE STANDARD RMARKDOWN YAML IN BOOKDOWN
---
title: "`sl3`: Modern Super Learning with Pipelines"
author: ""
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: template_refs.bib
---
-->

## notes
character classes

cv-superlearner -- not cv the metalearner step which is theoretically fine if
the metalearner is not too data adaptive.  
the loss is evaluated at the cv preds

imputation
rocr in ggplot

make it a screener as well

## Learning Objectives
By the end of this lesson you will be able to:
1. Select the metalearner that is appropriate for the functional parameter to be
estimated.
2. Design a cross-validation scheme.
3. Assemble an ensemble of learners based on the properties that identify what
features they support.
4. Customize learner hyperparameters to incorporate a diversity of different
settings.
6. Select a subset of available covariates and pass only those variables to the
modeling algorithm.
7. Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.
8. Interpret the super learner fit and rationalize the need to remove bias from
the super learner to make an optimal bias–variance tradeoff for the parameter of
interest.

## Introduction to `sl3`
In previous chapters, we introduced the road map for targeted learning as a
general template to translate real-world data applications into formal
statistical estimation problems. The first steps of this roadmap define the
*statistical estimation problem*, which establish (1) data as a realization of a
random variable, or equivalently, an outcome of a particular experiment; (2) a
statistical model representing the true knowledge about the data-generating
experiment; and (3) translation of the scientific question, which is often
causal, into a target parameter. Note that step (3) might require establishing
identifiability of the target quantity from the observed data distribution under
possible non-testable assumptions that are not necessarily believed to be
reasonable. In any case, the target quantity does have a valid statistical
interpretation.

Now that we have defined the statistical estimation problem, we are ready
construct the TMLE; an asymptotically efficient substitution estimator of this
target quantity. The first step in this estimation procedure is an initial
estimate of the data-generating distribution, or the relevant part of this
distribution that is needed to evaluate the target parameter. For this initial
estimation, we use the super learner. It is a loss-function-based tool that
allows us to obtain the best prediction of our target parameter based on a
weighted average of a library of machine learning algorithm. This library of
machine learning algorithms consists of functions that we think might be
consistent with the true data-generating function.


e need to "learn"

, we need to "learn"
from our data what the maximally unbiased and semiparametric efficient normally
distributed estimator of our target parameter is. “Learning” is the process
that attempts to provide the *best* estimate of our target parameter from a
library of guesses. What is the best? It is the estimator that is closest to
that which we would have derived had we known the true data-generating
distribution.
* What is the library of guesses? It is the collection of “models” that we
think might be consistent with the true data-generating function.
* What is the super learner? It is the loss-function-based tool that allows us
to obtain the best prediction of our target parameter based on a weighted
average of our guesses.

We introduce the first step of this estimation procedure using the WASH benefits
study example from Chapter 2.


## Introduction

In \@ref(intro), we introduced the road map for targeted learning as a
general template to translate real-world data applications into formal
statistical estimation problems. The first steps of this roadmap define the
*statistical estimation problem*, which establish (1) data as a realization of a
random variable, or equivalently, an outcome of a particular experiment; (2) a
statistical model representing the true knowledge about the data-generating
experiment; and (3) translation of the scientific question, which is often
causal, into a target parameter. Note that step (3) might require establishing
identifiability of the target quantity from the observed data distribution under
possible non-testable assumptions that are not necessarily believed to be
reasonable. In any case, the target quantity does have a valid statistical
interpretation and, now that we have defined the statistical estimation problem,
we are ready construct the TMLE; an asymptotically efficient substitution
estimator of this target quantity. The first step in this estimation procedure
is an initial estimate of the data-generating distribution, or the relevant part
of this distribution that is needed to evaluate the target parameter. For this
initial estimation, we use the Super Learner @van2007super. The Super Learner
provides an important step in creating a robust estimator. It is a
loss-function-based tool that uses V-fold cross-validation to obtain the best
prediction of our target parameter based on a weighted average of a library of
machine learning algorithms @wolpert1992stacked. This library of machine
learning algorithms consists of functions (“learners” in the `sl3` nomenclature)
that we think might be consistent with the true data-generating distribution.
The Super Learner has been proven to be asymptotically as accurate as the best
possible prediction algorithm that is tested @van2003unified.

## Background

A loss function $L$ is defined as a function of the observed data and a
candidate parameter value $\psi$, which has unknown true value $\psi_0$,
$L(\psi)(O)$. We can estimate the loss by substituting the empirical
distribution $P_n$ for the true (but unknown) distribution of the observed data
$P_0$. A valid loss function will have expectation (risk) that is minimized at
the true value of the parameter $\psi_0$. For example, the conditional mean
minimizes the risk of the squared error loss. Thus, it is a valid loss function
when estimating the conditional mean.

The cross-validated risk of a candidate estimator of the conditional mean of the
outcome, given the treatment and covariates, is defined as the empirical mean
over a validation sample of the loss of the candidate estimator fitted on the
training sample, averaged across different spits of the sample in a validation
and training sample.

A typical way to obtain such sample splits is so called V-fold cross-validation
in which one first partitions the sample in V subsets of equal size, and each of
the V subsets plays the role of a validation sample while its complement of
V − 1 subsets equals the corresponding training sample. Thus, V-fold cross-validation
results in V sample splits into a validation sample and corresponding training sample.

Given a library of candidate estimators, the Super Learner selects the estimator
that minimizes the cross-validated risk over all the candidate estimators. This
selected estimator is now applied to the whole sample to give our final estimate.
One can enrich the collection of candidate estimators by taking any weighted
combination of an initial library of candidate estimators, thereby generating
a whole parametric family of candidate estimators.

## Study Example

We introduce Super Learning with `sl3` using the WASH benefits study example.
Recall that our outcome of interest is the weight-for-height z-score `whz`, the
treatment is the randomized dietary intervention `tr`, and we consider all
other baseline covariates that we believe may confound the effect of treatment
on the weight-for-height z-score.

## Modern Super (Machine) Learning

### The Basics

First, we will load the relevant `R` packages and set a seed.

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(sl3)
library(SuperLearner)
library(origami)
library(here)
set.seed(7194)
```
We begin by illustrating the default functionality of the Super Learner
algorithm as implemented in `sl3`. Using the WASH data, we are interested in
predicting weight-for-height z-score `whz` using the available covariate data.

```{r data}
# load data set and take a peek
wash_data <- fread(here("..", "washb_data", "washb_data.csv"))
head(wash_data)
```
To define the machine learning problem (predict weight-for-height z-score
`whz` using the available covariate data), we need to create an `sl3_Task`
object. The `sl3_Task` keeps track of the roles the variables play in the machine
learning problem, the data, and any metadata (e.g., observational-level weights,
id, offset).

```{r task}
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(wash_data)[-which(names(wash_data) == outcome)]

# create the sl3 task
task <- make_sl3_Task(data = wash_data, covariates = covars, outcome = outcome)
```
The `make_sl3_Task` method is used create a new `sl3_Task`, which we called
`task`. Here, we also specified the underlying data and vectors indicating which
variables to use as covariates and outcome. Let's examine this object:

```{r task-examine}
task
```
We just demonstrated a simple use of `make_sl3_Task`, but `make_sl3_Task`
supports a range of options to facilitate proper articulation of more advanced
specifications. These additional features are documented in the help for
`sl3_Task`.

Now that we have defined our machine learning problem by making the task, we are
ready to define the machine learning algorithms. Learners have properties that
indicate what features they support. Use `sl3_list_properties()` to get a list
of all properties supported by at least one learner.

```{r list_properties}
sl3_list_properties()
```
Since we have a continuous outcome, we may identify the learners that support
this outcome type with `sl3_list_learners()`.

```{r list_learners}
sl3_list_learners(c("continuous"))
```

Now that we have an idea of some learners, we can construct them by the
`make_learner` function.

```{r baselearners}
# choose base learners
lrnr_glm <- make_learner(Lrnr_glm)
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_ranger <- make_learner(Lrnr_ranger)
lrnr_glmnet <- make_learner(Lrnr_glmnet)
lrnr_xgboost <- make_learner(Lrnr_xgboost)
```
Any learner can be used as a meta-learner and we will fit a non-negative least
squares meta-learner using `Lrnr_nnls`.

```{r metalearner}
# choose metalearner
metalearner <- make_learner(Lrnr_nnls)
```

```{r metalearner}
lrnr_xgboost10 <- make_learner(Lrnr_xgboost, nrounds = 10)
lrnr_svm <- make_learner(Lrnr_svm)
lrnr_polymars <- Lrnr_pkg_SuperLearner$new("SL.polymars")
lrnr_gam <- Lrnr_pkg_SuperLearner$new("SL.gam")
lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
lrnr_svm2 <- Lrnr_pkg_SuperLearner$new("SL.svm")

stack <- make_learner(Stack, lrnr_glm, lrnr_mean, lrnr_ranger,
                      lrnr_glmnet,lrnr_xgboost10, lrnr_svm, lrnr_xgboost,
                      lrnr_polymars, lrnr_gam, lrnr_bayesglm, lrnr_svm2)

Metalearner
```


```{r sl}
# run sl and predict on raw data
sl <- Lrnr_sl$new(learners = stack, metalearner = metalearner)
sl_fit <- sl$train(task)
sl_preds <- sl_fit$predict()

CV_Lrnr_sl

methods and fields
sl_fit$cv_risk
```

### Extensions

4. Customize learner hyperparameters to incorporate a diversity of different
settings.

6. Select a subset of available covariates and pass only those variables to the
modeling algorithm.

### Evaluation

7. Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.

## Interpretation
The super learner fit is the optimal fit of the overall prediction function, an
estimator that make the optimal bias–variance tradeoff with respect to the
loss-based dissimilarity between the super learner and the truth. To make an
optimal bias–variance tradeoff for the parameter of interest, we need to remove
bias from the super learner.


## Summary


## References


## Appendix: More advanced extensions of `sl3`

### Creating a new learner

### Variable importance
```{r varimp}
get_risk_diff <- function(dataset){
  # read the data

  dataset <- here("washb_data","washb_data.csv")
  data <- fread(dataset)
  data <- data.frame(data)
  cols_num <- c(colnames(data[c(1,4:5,7,9,11:12)]))
  data[cols_num] <- lapply(data[cols_num], as.numeric)
  cols_fac <- c(colnames(data[c(2:3,6,8,10,13:28)]))
  data[cols_fac] <- lapply(data[cols_fac], as.factor)
  data <- data.frame(data)
  # choose all the covs and the outcome
  outcome <- "whz"
  covars <- colnames(data)[-which(names(data) == outcome)]

  # create the sl3 task
  task <- make_sl3_Task(data = data, covariates = covars, outcome = outcome)

  # choose base learners
  lrnr_glm <- make_learner(Lrnr_glm)
  lrnr_mean <- make_learner(Lrnr_mean)
  lrnr_ranger <- make_learner(Lrnr_ranger)
  lrnr_glmnet <- make_learner(Lrnr_glmnet)
  lrnr_xgboost <- make_learner(Lrnr_xgboost)
  lrnr_xgboost10 <- make_learner(Lrnr_xgboost, nrounds = 10)
  lrnr_svm <- make_learner(Lrnr_svm)
  lrnr_polymars <- Lrnr_pkg_SuperLearner$new("SL.polymars")
  lrnr_gam <- Lrnr_pkg_SuperLearner$new("SL.gam")
  lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
  lrnr_svm2 <- Lrnr_pkg_SuperLearner$new("SL.svm")

  # stack them together
  stack <- make_learner(Stack, lrnr_glm, lrnr_mean, lrnr_ranger,
                        lrnr_glmnet,lrnr_xgboost10, lrnr_svm, lrnr_xgboost,
                        lrnr_polymars, lrnr_gam, lrnr_bayesglm, lrnr_svm2)

  # choose metalearner
  metalearner <- make_learner(Lrnr_nnls)

  # run sl and predict on raw data
  sl <- Lrnr_sl$new(learners = stack, metalearner = metalearner)
  sl_fit <- sl$train(task)
  sl_preds <- sl_fit$predict()
  risk <- mean(loss_squared_error(task$Y, sl_preds))

  risk_diffs <- lapply(covars, function(x){
    # scramble cov column and give it the same name as the raw cov col
    scrambled_col <- data.table(sample(unlist(data[,x, with = FALSE]),
                                       nrow(data)))
    names(scrambled_col) <- x

    # replace raw col with scrambled col in the task
    scrambled_col_names <- task$add_columns(scrambled_col)
    scrambled_col_task <- task$next_in_chain(column_names = scrambled_col_names)

    # obtain preds on the scrambled col task
    scrambled_sl_preds <- sl_fit$predict_fold(scrambled_col_task)

    # risk on scrambled col task
    risk_scrambled <- mean(loss_squared_error(task$Y,scrambled_sl_preds))

    # calculate risk difference
    rd <- risk_scrambled - risk
    return(rd)
  })

  names(risk_diffs) <- covars
  results <- data.table(cov = rep(names(risk_diffs), sapply(risk_diffs,length)),
                        risk_diff = unlist(risk_diffs))
  # arrange results in by increasing importance               
  results_ordered <- arrange(results, risk_diff)                      
  return(results_ordered)
}

Fitting likeli

```
