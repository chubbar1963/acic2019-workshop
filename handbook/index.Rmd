---
title: "The `tlverse` Software Ecosystem for Causal Inference"
subtitle: "2019 Atlantic Causal Inference Conference"
author: "Mark van der Laan, Alan Hubbard, Jeremy Coyle, Nima Hejazi, Ivana
  Malenica, Rachael Phillips"
date: "updated: `r format(Sys.time(), '%B %d, %Y')`"
documentclass: book
output: bookdown::gitbook
site: bookdown::bookdown_site
bibliography: [book.bib, packages.bib]
biblio-style: apalike
fontsize: '12pt, krantz2'
monofont: "Source Code Pro"
monofontoptions: "Scale=0.7"
link-citations: yes
links-as-notes: true
colorlinks: yes
lot: yes
lof: yes
always_allow_html: yes
url: 'https\://tlverse.org/acic2019-workshop/'
github-repo: tlverse/acic2019-workshop
graphics: yes
description: "An open-source and fully-reproducible electronic set of teaching
  materials accompanying a full-day short-course on applying the Targeted
  Learning methodology in practice using the [`tlverse` software
  ecosystem](https://github.com/tlverse)."
#cover-image: "img/tlverse_book_cover.png"
#apple-touch-icon: "img/logos/favicons/apple-touch-icon.png"
favicon: "img/logos/favicons/favicon.png"
---


```{r set-options, include=FALSE}
# Set output options
if (knitr:::is_html_output()) {
  options(width = 80)
}
if (knitr:::is_latex_output()) {
  options(width = 65)
}
options(digits = 7, bookdown.clean_book = TRUE, knitr.kable.NA = "NA")
knitr::opts_chunk$set(
  tidy = FALSE,
  out.width = "\textwidth",
  fig.align = "center",
  comment = NA
)
```

```{r pkg-bib, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "packages.bib")
```

# Preface {-}

<img style="float: left; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/tlverse-logo.svg" width="30%" height="30%">
<img style="float: center; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/Rlogo.svg" width="30%" height="30%">
<img style="float: right; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/vdl-logo-transparent.svg" width="30%" height="30%">
<p style="clear: both;">
<br>

This is an open source and fully-reproducible electronic handbook accompanying a
full-day short-course on applying the targeted learning methodology in practice
using the [`tlverse` software ecosystem](https://github.com/tlverse), given at
the [2019 Atlantic Causal Inference
Conference](https://mcgill.ca/epi-biostat-occh/news-events/atlantic-causal-inference-conference-2019) in
Montréal, Québec, Canada on 22 May 2019.

<img style="float: center; margin-right: 1%; margin-bottom: 0.05em"
     src="img/misc/ad_workshop.png" width="100%" height="100%">
<br>

## About this workshop {-}

This full-day workshop will provide a comprehensive introduction to the field of
targeted learning for causal inference and the corresponding [`tlverse`
software ecosystem](https://github.com/tlverse). In particular, we will focus on
targeted minimum loss-based estimators of causal effects, including those of
static, dynamic, optimal dynamic, and stochastic interventions. These multiply
robust, efficient plug-in estimators use state-of-the-art, ensemble machine
learning tools to flexibly adjust for confounding while yielding valid
tatistical inference. We will discuss the utility of this robust estimation
strategy in comparison to conventional techniques, which often rely on
restrictive statistical models and may therefore lead to severely biased
inference. In addition to discussion, this workshop will incorporate both
interactive activities and hands-on, guided `R` programming exercises, to allow
participants the opportunity to familiarize themselves with methodology and
tools that will translate to real-world causal inference analyses. It is highly
recommended for participants to have an understanding of basic statistical
concepts such as confounding, probability distributions, confidence intervals,
hypothesis tests, and regression. Advanced knowledge of mathematical statistics
may be useful but is not necessary. Familiarity with the `R` programming language
will be essential.

## Outline {-}

This is a full-day (6-hour) workshop, featuring modules that introduce distinct
causal questions, each motivated by a case study, alongside statistical
methodology and software for assessing the causal claim of interest. A sample
schedule may take the form:

* 09:00AM--09:15AM: [Why we need a statistical
    revolution](https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/)
* 09:15AM--09:45AM: The Roadmap, and the [WASH
    Benefits](http://www.washbenefits.net/) data
* 09:45AM--10:00AM: Introduction to the [`tlverse` software
    ecosystem](https://tlverse.org)
* 10:00AM--10:30AM: Morning coffee break
* 10:30AM--11:15AM: Super (machine) learning with the
    [`sl3`](https://github.com/tlverse/sl3) package
* 11:15AM--12:00PM: Targeted learning for causal inference with the
    [`tmle3`](https://github.com/tlverse/tmle3) package
* 12:00PM--1:00PM: Lunch break
* 01:00PM--02:00PM: Optimal treatment regimes and the
    [`tmle3mopttx`](https://github.com/tlverse/tmle3mopttx) package
* 02:00PM--02:30PM: Afternoon coffee break
* 2:30PM--3:30PM: Stochastic treatment regimes and the
    [`tmle3shift`](https://github.com/tlverse/tmle3shift) package
* 03:30PM--4:00PM: _Coda_: [Why we need a statistical
    revolution](https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/)

## About the instructors {-}

### Mark van der Laan {-}

Mark van der Laan, Ph.D., is Professor of Biostatistics and Statistics at UC
Berkeley. His research interests include statistical methods in computational
biology, survival analysis, censored data, adaptive designs, targeted maximum
likelihood estimation, causal inference, data-adaptive loss-based learning, and
multiple testing. His research group developed loss-based super learning in
semiparametric models, based on cross-validation, as a generic optimal tool for
the estimation of infinite-dimensional parameters, such as nonparametric density
estimation and prediction with both censored and uncensored data. Building on
this work, his research group developed targeted maximum likelihood estimation
for a target parameter of the data-generating distribution in arbitrary
semiparametric and nonparametric models, as a generic optimal methodology for
statistical and causal inference. Most recently, Mark's group has focused in
part on the development of a centralized, principled set of software tools for
targeted learning, the `tlverse`. For more information, see
https://vanderlaan-lab.org.

### Alan Hubbard {-}

Alan Hubbard, Ph.D., is Professor of Biostatistics, former head of the Division
of Biostatistics at UC Berkeley, and head of data analytics core at UC
Berkeley's SuperFund research program. His current research interests include
causal inference, variable importance analysis, statistical machine learning,
estimation of and inference for data-adaptive statistical target parameters, and
targeted minimum loss-based estimation. Research in his group is generally
motivated by applications to problems in computational biology, epidemiology,
and precision medicine.

### Jeremy Coyle {-}

Jeremy Coyle, Ph.D., is a consulting data scientist and statistical programmer,
currently leading the software development effort that has produced the
`tlverse` ecosystem of R packages and related software tools. Jeremy earned his
Ph.D. in Biostatistics from UC Berkeley in 2016, primarily under the supervision
of Alan Hubbard.

### Nima Hejazi {-}

Nima is a Ph.D. candidate in biostatistics with a designated emphasis in
computational and genomic biology, working jointly with Mark van der Laan and
Alan Hubbard. Nima is affiliated with UC Berkeley's Center for Computational
Biology and NIH Biomedical Big Data training program. His research interests
span causal inference, nonparametric inference and machine learning, targeted
loss-based estimation, survival analysis, statistical computing, reproducible
research, and high-dimensional biology. He is also passionate about software
development for applied statistics, including software design, automated
testing, and reproducible coding practices. For more information, see
https://nimahejazi.org.

### Ivana Malenica {-}

Ivana is a Ph.D. student in biostatistics advised by Mark van der Laan. Ivana is
currently a fellow at the Berkeley Institute for Data Science, after serving as
a NIH Biomedical Big Data and Freeport-McMoRan Genomic Engine fellow. She earned
her Master's in Biostatistics and Bachelor's in Mathematics, and spent some time
at the Translational Genomics Research Institute. Very broadly, her research
interests span non/semi-parametric theory, probability theory, machine learning,
causal inference and high-dimensional statistics. Most of her current work
involves complex dependent settings (dependence through time and network) and
adaptive sequential designs.

### Rachael Phillips {-}

Rachael is a Ph.D. student in biostatistics, advised by Alan Hubbard and Mark
van der Laan. She has an M.A. in Biostatistics, B.S. in Biology with a Chemistry
minor and a B.A. in Mathematics with a Spanish minor. Her research is applied,
and specific to human health. Motivated by issues arising in healthcare, Rachael
leverages strategies rooted in causal inference and nonparametric estimation to
build clinician-tailored, machine-driven solutions. Her accompanying statistical
interests include high-dimensional statistics and experimental design. She is
also passionate about free, online-mediated education. She is affiliated with
the UC Berkeley Center for Computational Biology, NIH Biomedical Big Data
Training Program, and Superfund Research Program.

## Important links

**Workshop surveys**
These pre- and post-workshop surveys help us ensure the effectiveness of our
teaching methodology.

* [pre-workshop survey](https://forms.gle/u6iZHYjd81RBwQVv7)
* [post-workshop survey]()


**Etherpad**
We will use this Etherpad for chatting, taking notes, and sharing URLs and bits
of code.

* [https://etherpad.net/p/acic2019-tlverse](https://etherpad.net/p/acic2019-tlverse)

**Code**
`R` script files for each section of the workshop are available via the GitHub
repository for the short course. See
https://github.com/tlverse/acic2019-workshop/tree/master/handbook/R

## Setup instructions

### R and RStudio

**R** and **RStudio** are separate downloads and installations. R is the
underlying statistical computing environment. RStudio is a graphical integrated
development environment (IDE) that makes using R much easier and more
interactive. You need to install R before you install RStudio.

#### Windows

##### If you already have R and RStudio installed

* Open RStudio, and click on "Help" > "Check for updates". If a new version is
  available, quit RStudio, and download the latest version for RStudio.
* To check which version of R you are using, start RStudio and the first thing
  that appears in the console indicates the version of R you are
  running. Alternatively, you can type `sessionInfo()`, which will also display
  which version of R you are running. Go on
  the [CRAN website](https://cran.r-project.org/bin/windows/base/) and check
  whether a more recent version is available. If so, please download and install
  it. You can [check here](https://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-do-I-UNinstall-R_003f)
  for more information on how to remove old versions from your system if you
  wish to do so.

##### If you don't have R and RStudio installed

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/windows/base/release.htm).
* Run the `.exe` file that was just downloaded
* Go to the [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select **RStudio x.yy.zzz - Windows
  XP/Vista/7/8** (where x, y, and z represent version numbers)
* Double click the file to install it
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.


#### macOS

##### If you already have R and RStudio installed

* Open RStudio, and click on "Help" > "Check for updates". If a new version is
  available, quit RStudio, and download the latest version for RStudio.
* To check the version of R you are using, start RStudio and the first thing
  that appears on the terminal indicates the version of R you are running.
  Alternatively, you can type `sessionInfo()`, which will also display which
  version of R you are running. Go on the [CRAN
  website](https://cran.r-project.org/bin/macosx/) and check whether a more
  recent version is available. If so, please download and install it.

##### If you don't have R and RStudio installed

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/macosx).
* Select the `.pkg` file for the latest R version
* Double click on the downloaded file to install R
* It is also a good idea to install [XQuartz](https://www.xquartz.org/) (needed
  by some packages)
* Go to the [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select **RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)**
  (where x, y, and z represent version numbers)
* Double click the file to install RStudio
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.


#### Linux

* Follow the instructions for your distribution
  from [CRAN](https://cloud.r-project.org/bin/linux), they provide information
  to get the most recent version of R for common distributions. For most
  distributions, you could use your package manager (e.g., for Debian/Ubuntu run
  `sudo apt-get install r-base`, and for Fedora `sudo yum install R`), but we
  don't recommend this approach as the versions provided by this are
  usually out of date. In any case, make sure you have at least R 3.3.1.
* Go to the
  [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select the version that matches your distribution, and
  install it with your preferred method (e.g., with Debian/Ubuntu `sudo dpkg -i
  rstudio-x.yy.zzz-amd64.deb` at the terminal).
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.

These setup instructions are adapted from those written for [Data Carpentry: R
for Data Analysis and Visualization of Ecological
Data](http://www.datacarpentry.org/R-ecology-lesson/).
