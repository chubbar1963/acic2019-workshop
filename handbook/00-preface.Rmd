## Preface {-}

Scientific research is at a unique point in history. The need to improve rigor
and reproducibility in our field is greater than ever; corroboration moves
science forward, yet there is a growing alarm about results that cannot be
reproduced.

"One enemy of robust science is our humanity — our appetite for
being right, and our tendency to find patterns in noise, to see supporting
evidence for what we already believe is true, and to ignore the facts that do
not fit." @naturenews_2015.

"The key question we want to answer when seeing the results of any scientific
study is whether we can trust the data analysis." @peng2015reproducibility

Sadly at its current state, the culture of data analysis actually enables human
bias through model selection. Statistical models are often chosen based on the
p-values they yield, their convenience of implementation, and/or an analysts
loyalty to a particular algorithm. This practice allows one to make arbitrary
modeling choices, even though these choices result in different answers to the
same research question. This presents a fundamental drive behind the epidemic
of false positives that scientific research is suffering from. More than ever
we need training of robust methodologies that regulate these
all-too-human biases (e.g., hindsight bias, confirmation
bias, and outcome bias) and prevent the errors they cause. Consequences of not
meeting this need will result in further decline in the rate of scientific
progression, the reputation of the sciences, and the public’s trust in its
findings @munafo2017manifesto @naturenews2_2015.

Our team at The University
of California, Berkeley, is uniquely positioned to provide such a training.
Spearheaded by Professor Mark van der Laan, and spreading rapidly by many of his
students and colleagues who have greatly enriched the field, the aptly named
“Targeted Learning” methodology targets the scientific question at hand and is
counter to the current culture of “convenience statistics” that
opens the door to biased estimation, misleading results, and false discoveries.
Capable of answering specific questions of interest based on real-world
observational and experimental data, Targeted Learning unifies desirable aspects
of algorithmic machine learning and causal inference to generate efficient and
trustworthy inferences. Targeted Learning restores the fundamentals that
formalized the field of statistics, such as the that facts that a statistical model
represents real knowledge about the experiment that generated the data, and a
target parameter represents what we are seeking to learn from the data as a
feature of the distribution that generated it. In this way, Targeted Learning
defines a truth and establishes a principled standard for estimation
@van2014entering. The objective for this handbook is to enhance the education
of students, researchers, professors, etc. to empower them with the necessary
knowledge and skills to utilize the sound research methodology of Targeted
Learning.

For any statistical methodology to be readily accessible in practice, it is
crucial that it is accompanied by robust user-friendly software. The `tlverse`
software ecosystem was developed to fulfill this need. Not only does this
software facilitate computationally reproducible and efficient analyses, it is
also a tool for Targeted Learning education since its workflow mirrors that of
the methodology. In particular, the `tlverse` paradigm does not focus on
implementing a specific estimator or a small set of related estimators ---
instead, the focus is on exposing the statistical framework of Targeted Learning
itself! All `R` packages in the `tlverse` ecosystem directly model
the key objects defined in the mathematical and theoretical framework of Targeted
Learning. In this handbook, the reader will embark on a journey through the
`tlverse`. Guided by `R` programming exercises, case studies, and
intuitive explanation readers will build a toolbox for applying the Targeted
Learning statistical methodology, thereby increasing accessibility of this
statistical approach and philosophy. The reader need not be a fully trained
statistician to begin understanding and applying these methods. However, we do
recommend ___ before




<!--
However, a statistical model is a
representation of the data-generating process; defined as the set of
distributions that might have generated the data. All hypothesis tests and
estimators are derived from statistical models, so appropriately defining them
is pivotal.
election is pivotal. should be taken with case  real knowledge about the
experiment that generated the dataFrequently, odels are chosen based on (1) how low the
resulting p-values, (2) their software implementation is user-friendly, or (3)
the data analyst is devoted
convenient to implement (2) , so it is pivotal that we
  which are often chosen for their convenience restrictive so-called parametric
statistical models.
These models assume the distributions that generated the data have specific
forms and are not flexible enough to capture the true distribution of data. Many
accept that these models are wrong but researchers still employ them out of
convenience and the majority of statistical software .


What's more, these arbitrary choices  "art" of model selection
downplays the refitting of models  enables the statistics can be as a tool for confirmation.It's convenient and this is how we
learned to analyze data. This is what we
were taught to do. We are supported by statements such as "all models are wrong but some
are useful,” which allow a user to .

This practice is So convenient that

This lack of truth in current culture typically trumps trying to answer the real scientific question at hand and is  This presents a fundamental drive behind the epidemic of false positives that scientific research is suffering from [6]. “The human brain’s habit of finding what it wants to find is a key problem for research” [1]. Training of robust methods that avoid confirmation bias will lead to results being more reproducible. Our team at The University of California, Berkeley, is uniquely positioned to provide such a training. Spearheaded by Professor Mark van der Laan and spreading rapidly by many of his students and colleagues who have greatly enriched the field, the aptly named “Targeted Learning” analysis approach and philosophy targets the scientific question at hand and is counter to the current culture of “convenience statistics” that opens the door to biased estimation, misleading results, and false discoveries.


The foundation of this handbook is grounded in the general statistical
methodology and philosophy, “Targeted Learning”, developed by
Professor Mark van der Laan, at the University of California, Berkeley. Capable
of answering specific questions of interest based on real-world observational and
experimental data, Targeted Learning unifies desirable aspects of algorithmic
machine learning and causal inference to generate efficient and trustworthy
inferences. Targeted Learning restores the fundamentals that formalized the
field of statistics -- a statistical model represents real knowledge about the
experiment that generated the data, and a target parameter represents what we
are seeking to learn from the data as a feature of the distribution that
generated it. In this way, Targeted Learning defines a truth and establishes a
principled standard for estimation, while the current culture of analysis
typically defines a parameter as a coefficient in a misspecified parametric (or
other restrictive) statistical model. Unfortunately, the practice of data analysis
allows one to make arbitrary modeling choices, likely driven by results returned
by the model and human bias, even though these choices often result in different
answers to the same problem. This subjectivity presents a fundamental drive
behind the epidemic of false positives and lack of power to detect true
positives that scientific research is suffering from @van2014entering.

"The key question we want to answer when seeing the results of any scientific
study is whether we can trust the data analysis." @peng2015reproducibility

"One enemy of robust science is our humanity — our appetite for
being right, and our tendency to find patterns in noise, to see supporting
evidence for what we already believe is true, and to ignore the facts that do
not fit." @naturenews_2015.

We are faced with an urgent scientific need to enhance the reproducibility and
rigor of research and the current culture of data analysis enables a major
contributor of this crisis – human bias. Consequences of not meeting this need
will result in further decline in the rate of scientific
progression, the reputation of biomedical science, and the public’s trust in
its findings.

Training of robust methods that avoid confirmation bias will lead
to results being more reproducible and trustworthy. Our team at The University
of California, Berkeley, is uniquely positioned to provide such a training. The
objective for this handbook is to enhance the education of students,
researchers, professors, etc. to empower them with the necessary
knowledge and skills to utilize the sound research methodology of Targeted
Learning.

For any complex statistical methodology to be accessible in practice, it is
crucial that it is accompanied by robust software. The `tlverse`
software ecosystem was developed to fulfill this need. Not only does this
software facilitate computationally reproducible and efficient analyses, it is
also a tool for Targeted Learning education since its workflow mirrors that of
the methodology. In particular, the `tlverse` paradigm does not focus on
implementing a specific estimator or a small set of related estimators ---
instead, the focus is on exposing the statistical framework of Targeted Learning
itself! All `R` packages in the `tlverse` ecosystem directly model
the key objects defined in the mathematical and theoretical framework of Targeted
Learning.

In this handbook, the reader will embark on a journey through the
`tlverse`. Guided by `R` programming exercises, case studies, and
intuitive explanation readers will build a toolbox for applying the Targeted
Learning statistical methodology, thereby increasing accessibility of this
statistical approach and philosophy. The reader need not be a fully trained
statistician to begin understanding and applying these methods. However, we do
recommend ___ before -->
