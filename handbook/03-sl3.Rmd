# Modern Super (Machine) Learning with `sl3`

_Rachael Phillips_, based on the [`sl3` package](https://github.com/tlverse/sl3)
by _Jeremy Coyle, Nima Hejazi, Ivana Malenica, and Oleg Sofrygin_

Updated: `r Sys.Date()`

## Learning Objectives
By the end of this lesson you will be able to:

1. Assemble an ensemble of learners based on the properties that identify what
features they support.
2. Customize learner hyperparameters to incorporate a diversity of different
settings.
3. Select a subset of available covariates and pass only those variables to the
modeling algorithm.
4. Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.
5. Calculate `sl3` variable importance metrics.
6. Interpret the discrete and continuous super learner fits.
7. Rationalize the need to remove bias from the super learner to make an optimal
bias–variance tradeoff for the parameter of interest.

## Introduction

Now that we have defined the statistical estimation problem, we are ready
construct the TMLE; an asymptotically efficient substitution estimator of this
target quantity. The first step in this estimation procedure is an initial
estimate of the data-generating distribution, or the relevant part of this
distribution that is needed to evaluate the target parameter. For this initial
estimation, we use the super learner @van2007super, an important step in
creating a robust estimator.

#### Super learner

* Loss-function-based tool that uses V-fold cross-validation to obtain the best
prediction of the relevant part of the likelihood that's needed to evaluate
target parameter.

* Requires expressing the estimand as the minimizer of an expected loss, and
proposing a library of algorithms ("learners" in `sl3` nomenclature) that we
think might be consistent with the true data-generating distribution.

* Proven to be asymptotically as accurate as the best possible prediction
algorithm that is tested @vdl2003unified @van2006oracle.

* The *discrete super learner*, or cross-validated selector, is the algorithm in
the library that minimizes the V-fold cross-validated empirical risk.

* The *continuous super learner* is a weighted average of the library of
algorithms, where the weights are chosen to minimize the V-fold cross-validated
empirical risk of the library. Restricting the weights ("metalearner" in `sl3`
nomenclature) to be positive and sum to one (convex combination) has been shown
to improve upon the discrete super learner @polley2010super @van2007super.

## Basic Implementation

We begin by illustrating the basic functionality of the super learner
algorithm as implemented in `sl3`. `sl3` implementation consists of the
following steps:

0. Load the necessary libraries and data
1. Define the machine learning task
2. Make a super learner by creating library of base learners and a metalearner
3. Train the super learner on the machine learning task
4. Obtain predicted values

### WASH Benefits Study Example

Using the WASH data, we are interested in predicting weight-for-height z-score
`whz` using the available covariate data. Let's begin!

**0. Load the necessary libraries and data**

```{r setup, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(data.table)
library(sl3)
library(SuperLearner)
library(origami)
library(knitr)

set.seed(7194)

# load data set and take a peek
washb_data <- fread(here("data", "washb_data.csv"), stringsAsFactors = TRUE)
head(washb_data, 3) %>% kable(format = 'latex', digits = 3)
```

**1. Define the machine learning task**

To define the machine learning **"task"** (predict weight-for-height z-score
`whz` using the available covariate data), we need to create an `sl3_Task`
object. The `sl3_Task` keeps track of the roles the variables play in the
machine learning problem, the data, and any metadata (e.g., observational-level
weights, id, offset).

```{r task}
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]

# create the sl3 task
washb_task <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome
)

# examine it
washb_task
```

**2. Make a super learner**

Now that we have defined our machine learning problem with the task, we are
ready to **"make"** the machine learning algorithms.

Learners have properties that indicate what features they support. We may use
`sl3_list_properties()` toget a list of all properties supported by at least
one learner.

```{r list_properties}
sl3_list_properties()
```
Since we have a continuous outcome, we may identify the learners that support
this outcome type with `sl3_list_learners()`.

```{r list_learners}
sl3_list_learners(c("continuous"))
```

Now that we have an idea of some learners, we can construct them using the
`make_learner` function.

```{r baselearners}
# choose base learners
lrnr_glm <- make_learner(Lrnr_glm)
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_ranger <- make_learner(Lrnr_ranger)
lrnr_glmnet <- make_learner(Lrnr_glmnet)
```
In order to assemble the library of learners, we need to **"stack"** them
together. A `Stack` is a special learner and it has the same interface as all
other learners. What makes a stack special is that it combines multiple learners
by training them simultaneously, so that their predictions can be either
combined or compared.

```{r stack}
stack <- make_learner(
  Stack,
  lrnr_glm, lrnr_mean, lrnr_ranger, lrnr_glmnet
)
```
We're almost ready to super learn! Just a couple more necessary specifications.

We will fit a non-negative least squares metalearner using `Lrnr_nnls`. Note
that any learner can be used as a metalearner.

```{r metalearner}
metalearner <- make_learner(Lrnr_nnls)
```
Now that we have made a library/stack of base learners and a metalearner, we
are ready to make the super learner.

```{r make sl}
sl <- make_learner(Lrnr_sl,
                   learners = stack,
                   metalearner = metalearner
)
```

**3. Train the super learner on the machine learning task**

The super learner algorithm fits a metalearner on the validation-set
predictions in a cross-validated manner, thereby avoiding overfitting. This
procedure is referred to as the *continuous* super learner. The cross-validation
selector, or *discrete* super learner, is the base learner with the lowest
cross-validated risk.

Now we are ready to **"train"** our super learner on our `sl3_task` object.

```{r sl_basic}
sl_fit <- sl$train(washb_task)
```

**4. Obtain predicted values**

Now that we have fit the super learner, we are ready to obtain our predicted
values, and we can also obtain a summary of the results.

```{r sl_basic-summary}
sl_preds <- sl_fit$predict()
head(sl_preds)
sl_fit$print() %>% knitr::kable(format = "markdown", digits = 3)
```
### Extensions

**Customize Learner Hyperparameters**

We can customize learner hyperparameters to incorporate a diversity of different
settings. We can also include learners from the `SuperLearner` `R` package.

```{r extra-lrnr}
lrnr_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)
lrnr_ranger1k <- make_learner(Lrnr_ranger, num.trees = 1000)
lrnr_gam <- Lrnr_pkg_SuperLearner$new("SL.gam")
lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
```
Let's create a new stack with these new learners, so we may incorporate them in
a new super learner.

```{r new-stack}
new_stack <- make_learner(
  Stack,
  lrnr_glm, lrnr_mean, lrnr_ranger, lrnr_glmnet, lrnr_ranger1k, lrnr_ranger100,
  lrnr_gam, lrnr_bayesglm
)
```

**Screening covariates**

We can also select a subset of available covariates and pass only
those variables to the modeling algorithm.

Let's see how this works. Consider screening covariates based on their
correlation with the outcome (`cor.test` p-value $\leq 0.1), and `randomForest`
variable importance (top 10 most important variables).

```{r screeners}
screen_cor <- Lrnr_pkg_SuperLearner_screener$new("screen.corP")
#screen_rf <- Lrnr_pkg_SuperLearner_screener$new("screen.randomForest")
```
Now we need to **"pipe"**  only those selected covariates to the modeling
algorithm. To accomplish this, we need to make a `Pipeline`, which is a just
set of learners to be fit sequentially, where the fit from one learner is used
to define the task for the next learner.

```{r screeners-pipe}
cor_pipeline <- make_learner(Pipeline, screen_cor, new_stack)
#rf_pipeline <- make_learner(Pipeline, screen_rf, new_stack)
```
Lastly, we just have to stack all of these pipelines together, so we may use
them as base learners in our super learner.

```{r screeners-stack}
fancy_stack <- make_learner(Stack, cor_pipeline, new_stack)
```
Now we can Super Learn with this fancy base learner stack.

```{r sl_fancy, eval=FALSE}
# run sl and predict on WASH
sl_fancy <- Lrnr_sl$new(learners = fancy_stack, metalearner = metalearner)
sl_fancy_fit <- sl_fancy$train(washb_task)
#Failed on Lrnr_pkg_SuperLearner_screener_screen.randomForest
#Error in eval(predvars, data, env) :
#  object 'tr.Nutrition + WSH' not found
sl_preds <- sl_fancy_fit$predict()
sl_fancy_fit$print() %>% knitr::kable(format = "markdown", digits = 3)
```

**Cross-validated super learner**

We can cross-validate the super learner to see how well the super learner
performs on unseen data. This requires an “external” layer of cross-validation,
also called nested cross-validation, which involves setting aside a separate
holdout sample that we don’t use to fit the super learner. This
external cross validation procedure may also incorporate 10 folds, but this is
very time consuming. The default in `sl3` is ______.

```{r CVsl}
CVsl_fancy <- CV_lrnr_sl(sl_fit, washb_task, loss_squared_error)
CVsl_fancy %>% kable(format = 'latex', digits = 3)
```

**Variable importance**

Variable importance can be interesting and informative. Let's explore how it
works in `sl3`.

```{r varimp}
washb_varimp <- varimp(sl_fit, loss_squared_error, type = "permute")
washb_varimp %>% kable(format = 'latex', digits = 3)
```
This function calculates the risk difference between the learner fit with a
scrambled covariate and the learner fit with the true covariate, across all
covariates. In this manner, the larger the risk difference the more important
the variable is in the prediction.

## Exercise
### Predicting myocardial infarction with `sl3`

Using the `chspred` data, use `sl3` to predict myocardial infarcation (`mi`)
using the available covariate data.

Work with a buddy. You have 20 minutes.

In the etherpad, submit your group's answers to the following questions.

1. Which learner was the discrete super learner? What was the cross validated
risk of the discrete super learner?
2. What was the cross validated risk of the continuous super learner?
3. Did your group face any challenges?
