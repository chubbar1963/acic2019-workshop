# The Roadmap for Targeted Learning

## Learning Objectives
By the end of this chapter you will be able to:
1. Translate scientific questions to statistical questions.
2. Define a statistical model based on the knowledge of the data-generating
distribution. 

## Introduction
The roadmap of statistical learning is concerned with the translation from the
real-world data application into a mathematical and statistical formulation of
the actual estimation problem that needs to be solved. This involves data as a
random variable having a probability distribution, statistical knowledge
represented by a statistical model, statistical target parameter which
represents the question of interest, and the notion of an estimator and sampling
distribution of the estimator.  

## The Roadmap for Targeted Learning

1. Data as a random variable with a probability distribution, $O \sim P_0$.

$$O_1, ..., O_n \simiid P_0$$

The data set we're confronted with is the result of an experiment and we can
view the data as a random variable, $O$, because if we repeat the experiment
we would have a different realization of this experiment. In particular, if we
repeat the experiment many times we could learn the probability distribution,
$P_0$, of our data. So, the observed data $O$ with probability distribution
$P_0$ are $n$ independent identically distributed (i.i.d) observations of the
random variable $O; $O_1, \ldots, O_n$. Note that i.i.d. is a real assumption
and not all data are i.i.d. There are ways to handle non-i.i.d. data such as
establishing conditional independence, stratifying the data to create sets of
identically distributed data, etc. Researchers need to be absolutely clear
about what they actually know about the distributions that generate their data.
Unfortunately, communication between statisticians and researchers often gets
lost in translation and thereby misinterpreted. This is one reason why the
roadmap is so helpful -- it truly helps with this communication!

#### The empirical probability measure, $P_n$
Once we have $n$ of such i.i.d. observations we have an empirical probability
measure, $P_n$. The empirical probability measure is an approximation of the true
probability measure $P_0$ and it allows us to learn from our data. For example, we
can define the empirical probability measure of a set, $A$, to be the proportion
of observations which end up in $A$. That is,

$$P_n(A) = \frac{1}{n}\sum_{i=1}^{n}I(O_i \in A)$$

In order to start learning something, we need to ask *``What do we know about the
probability distribution of the data?"* This brings us to Step 2.

2. The statistical model, $P_0 \in \mathcal{M}$

The statistical model $\mathcal{M}$ is defined by the question we asked at the
end of Step 1. It is defined as the set of possible probability distributions
for our data. Often $\mathcal{M}$ is very large because statistical knowledge
is limited so $\mathcal{M}$ is often infinite dimensional. We call this a
non-parametric statistical model.\\

Alternatively, if the probability distribution of your data is described by a
finite number of parameters then the statistical model is parametric. This
means we believe our random variable we're observing on a unit like
blood pressure has e.g. a normal distribution with mean $\mu$ and variance
$\sigma^2$.  More formally, a parametric model is defined as:

$$\mathcal{M} = \{P_{\theta} : \theta \in \mathcal{R}^d \}$$

Sadly, it is all-too-common to assume the data-generating distributions have
specific forms when such knowledge is certainly not at hand. This lack of truth
in the current culture of analysis typically trumps trying to answer the real
scientific question at hand and is supported by statements such as
“All models are wrong but some are useful,” which allow a user to make arbitrary
choices even though these choices result in different answers to the same
estimation problem. The Targeted Learning methodology does not suffer from this
selection bias since it defines the statistical model as a representation for
the true data generating distribution that produced the observed data.

Our next question becomes, *``What are we trying to learn from the data?"*
This brings us to Step 3.

3. The statistical target parameter, $\Psi: \mathcal{M}\rightarrow\mathbb{R}$

The statistical target parameter, $\Psi$ is defined as a mapping from the
statistical model, $\mathcal{M}$ to the parameter space (i.e., a number),
$\mathcal{R}$. It is defined by the query mentioned at the end of Step 2.
Target parameters of interest represent questions, which are often causal.
Causal target parameters require .....\\

For example, say we observe a survival time on every subject and our question
of interest is "What's the probability that someone lives longer than five
years?" We have,

$$\Psi(P_0) = P_0(O > 5)$$

This answer to this question brings us to Step 4.

4. The estimand, $\Psi(P_0)$

The answer to the query asked in Step 3. This estimand is quantity we're really
trying to learn. Once we have defined $O$, $\mathcal{M}$ and $\Psi(P_0)$ we have
formally defined the statistical estimation problem.

#### After formally defining the statistical estimation problem

We use the data to estimate the estimand and hopefully the estimator has good
statistical properties to approximate the estimate. Additionally, we try to end
up with statistical inference (i.e. we try to quantify the uncertainty in our
estimator). We need statistical theory to guide us in the construction of our
estimators.

5. The estimator, $\hat{\Psi} : \mathcal{M}_{NP} \rightarrow \mathbb{R}^d$

To come up with a good estimand we need an estimator, an a-priori specified
algorithm defined as a mapping from the set of possible empirical distributions,
$P_n$, which live in a non-parametric statistical model, $\mathcal{M}_{NP}$
($P_n \in \mathcal{M}_{NP}$), to the parameter space for our parameter of
interest. It is a function that takes as input the observed data, a realization
of $P_n$, and gives as output a value in the parameter space.


6. The estimate, $\hat{\Psi}(P_n)$

The output mentioned at the end of Step 5 is defined as the estimate. It is a
function of the empirical probability distribution of the data that is an
element of the parameter space. If we plug in a realization of $P_n$ (based on
a sample size $n$ of the random variable $O$), we get back an estimate
$\hat{\Psi}(P_n)$ of the true parameter value $\Psi(P_0)$.\\

In order to have any hope of coming up with the quantification of the
uncertainty (i.e. statistical inference), we need to understand the sampling
distribution of our estimator. This brings us to step 7.

7. Sampling distribution of $\hat{\Psi}(P_n)$

The sampling distribution of $\hat{\Psi}(P_n)$ says that the estimator itself
is a random variable. So, if we repeat the experiment of drawing $n$
observations we would every time end up with a different realization of our
estimator and our estimator has a sampling/probability distribution. Hopefully
this sampling distribution can be theoretically validated to be approximately
normally distributed.

#### Statistical Inference

The \textbf{Central Limit Theorem} (CLT) allows us to make statements regarding
the \textit{sampling distribution of our estimator} approximating a normal
distribution when the sample size gets large enough. For large $n$ we have,

$$\hat{\Psi}(P_n) \sim N\Big( \Psi(P_0), \frac{\sigma^2}{n} \Big)$$

This permits statistical inference. Now we can quantify the uncertainty in our
estimator. For example, we can construct a 95\% confidence interval for our
estimand, $\Psi(P_0)$:

$$\hat{\Psi}(P_n) \pm 1.96 \Big( \frac{\sigma}{\sqrt{n}} \Big)$$

Note: we typically have to estimate the standard error,
$\frac{\sigma}{\sqrt{n}}$.\\

A 95\% confidence interval means that if we were to take 100 different samples
of size $n$ and compute a 95\% confidence interval for each sample then
approximately 95 of the 100 confidence intervals would contain the estimand,
$\Psi(P_0)$. More practically, this means that there is a 95\% probability
(or 95\% confidence) that the confidence interval procedure will contain the
true estimand. However, any single estimated confidence interval either will
contain the true estimand or will not.

## Summary

Data, $O$, is viewed as a random variable that has a probability distribution.
We often have $n$ units of independent identically distributed units with
probability distribution $P_0$ ($O_1, \ldots, O_n \simiid P_0$). We have
statistical knowledge about the experiment that generated this data. In other
words, we make a statement that the true data distribution $P_0$ falls in a
certain set called a statistical model, $\mathcal{M}$. Often these sets are very
large because statistical knowledge is very limited so these statistical models
are often infinite dimensional models. Our statistical query is, ``What are we
trying to learn from the data?" denoted by the statistical target parameter,
$\Psi$, which maps the $P_0$ into the estimand, $\Psi(P_0)$. At this point the
statistical estimation problem is formally defined and now we will need
statistical theory to guide us in the construction of estimators. There's a lot
of statistical theory we will review in this course that, in particular, relies
on the Central Limit Theorem, allowing us to come up with estimators that are
approximately normally distributed and also allowing us to come with statistical
inference (i.e. confidence intervals).
