# Stochastic Treatment Regimes

_Nima Hejazi, Jeremy Coyle, Mark van der Laan_

Updated: `r Sys.Date()`

## Learning Objectives
<!--- appears as "X.1: Learning Objectives" in the book, where X is the chapter
corresponding to stochastic interventions -->
1. ...
2. ...
3. ...
4. ...
5. ...

## Introduction to Stochastic Interventions

Stochastic treatment regimes present a relatively simple manner in which to
assess the effects of continuous treatments by way of parameters that examine
the effects induced by the counterfactual shifting of the observed values of a
treatment of interest. Here, we present an implementation of a new algorithm for
computing targeted minimum loss-based estimates of treatment shift parameters
defined based on a shifting function $d(A,W)$. For a technical presentation of
the algorithm, the interested reader is invited to consult @diaz2018stochastic.
For additional background on Targeted Learning and previous work on stochastic
treatment regimes, please consider consulting @vdl2011targeted,
@vdl2018targeted, and @diaz2012population.

## Background on Stochastic Interventions

TODO

## Data Structure and Notation

Consider $n$ observed units $O_1, \ldots, O_n$, where each random variable $O =
(W, A, Y)$ corresponds to a single observational unit. As discussed before, $W$
are baseline covariates, $A$ is a continuous-valued intervention, and $Y$ an
outcome of interest. We will consider a simple stochastic intervention that
considers counterfactual interventions defined through scalar shifts of the
observed/natural value of the intervention $A$. We'll start by considering a
simple additive shift $d(a,w) = a + \delta$, assuming support a.e. of
$\mathbb{P}(A \mid W)$. To ease violations of the positivity assumption, we may
consider extensions where $a \leq u(w) - \delta$ or $d(a, w) = a$ if $a \geq
u(w) - \delta$.

## Defining the Causal Effect of a Stochastic Intervention

Likelihood Factorization for the Full Data:
Let $q_{0, Y}$ be the conditional density of $Y$ given $(A, W)$ wrt dominating
measure $\xi$. Let $q_{0, A}$ be the conditional density of $A$ given $W$ wrt
dominating measure $\mu$. Let $q_{0, W}$ be the density of $W$ wrt dominating
measure $\nu$. Then, for $p_0^O$, density of $O$ wrt the product measure,
density evaluated on a particular observation $o$:
\begin{equation*}\label{likelihood_factorization}
  p_0^O(x) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w).
\end{equation*}

Use a nonparametric structural equation model (NPSEM) to describe
generation of $O$ @pearl2009causality, specifically
\begin{align*}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y)
\end{align*}

NPSEM parameterizes $p_0^O$ in terms of the distribution of RVs $(O, U)$
modeled by this system of equations.

Implies a model for the distribution of counterfactual RVs generated by
interventions on the data-generating process.
Notation: let $f_W$, $f_A$, $f_Y$ be deterministic functions, and $U_W$,
$U_A$, $U_Y$ exogenous RVs.


\textit{Stochastic interventions} modify the value $A$ would naturally
assume, $f_A(W, U_A)$, by drawing from a modified intervention distribution
$G^{\star}(\cdot \mid W)$ so that the new value $A^{\star} \sim
G^{\star}(\cdot \mid W)$.

This generates a counterfactual RV, with distribution $P_{0}^d$,
$Y_{d(A, W)} := f_Y(d(A,W), W, U_Y) \equiv Y_{G^{\star}} :=
f_Y(A^{\star}, W, U_Y)$. We estimate $\psi_{0, d} :=
\mathbb{E}_{P_0^d}\{Y_{d(A,W)}\}$, mean of $Y_{d(A, W)}$.

Starts with Mark and Ivan's simple stochastic shift.
Extensions to modified treatment policies.
The new value of $A$ may be denoted $A^{\star} \sim G^{\star}(\cdot \mid W)$,
where $A^{\star} = d(W, U^{\star})$ for a rule $d$ and random error $U^{\star}$.

#### Literature: @diaz2012population

- \textit{Proposal:} Evaluate outcome under an altered
    \textit{intervention distribution} --- e.g.,
    $P_{\delta}(g_0)(A = a \mid W) = g_0(a - \delta(W) \mid W)$.
- Identification conditions for a statistical parameter of the
    counterfactual outcome $\psi_{0,d}$ under such an intervention.
- Show that the causal quantity of interest $\E_0 \{Y_{d(A, W)}\}$ is
    identified by a functional of the distribution of $O$:
  \begin{align*}\label{eqn:identification2012}
    \psi_{0,d} = \int_{\mathcal{W}} \int_{\mathcal{A}} & \mathbb{E}_{P_0}
     \{Y \mid A = d(a, w), W = w\} \cdot \\ &q_{0, A}^O(a \mid W = w) \cdot
     q_{0, W}^O(w) d\mu(a)d\nu(w)
  \end{align*}
- Provides a derivation based on the efficient influence function (EIF) with
  respect to the nonparametric model $\mathcal{M}$.

- The identification result allows us to write down the causal quantity
  of interest in terms of a functional of the observed data.
- Key innovation: loosening standard assumptions through a change in
  the observed intervention mechanism.
- Problem: globally altering an intervention mechanism does not
  necessarily respect individual characteristics.
- The authors build IPW, A-IPW, and TML estimators, comparing the three
  different approaches.
- IMPORTANT: gives the g-computation formula for identification of this
  estimator from the observed data structure.


#### Literature: @diaz2018stochastic

- Builds on the original proposal, accommodating shifts  $d(A,W)$ proposed
  after their earlier work.
- To protect against positivity violations, considers a specific shifting
  mechanism:
  \begin{equation*}\label{shift_intervention}
    d(a, w) =
      \begin{cases}
        a + \delta, & a + \delta < u(w) \\
        a, & \text{otherwise}
      \end{cases}
  \end{equation*}
- Proposes an improved "1-TMLE" algorithm, with a single auxiliary
  ("clever") covariate for constructing the TML estimator.


Identification: From Causal Inference to Statistics

- (Assumption 1) _Consistency_: $Y^{d(a_i, w_i)}_i = Y_i$ in the event
  $A_i = d(a_i, w_i)$, for $i = 1, \ldots, n$
- (Assumption 2) _SUTVA_: $Y^{d(a_i, w_i)}_i$ does not depend on
  $d(a_j, w_j)$ for $i = 1, \ldots, n$ and $j \neq i$, or lack of interference
  [@rubin1978bayesian; @rubin1980randomization]
- (Assumption 3) _Strong ignorability_: $A_i \indep Y^{d(a_i, w_i)}_i
  \mid W_i$, for $i = 1, \ldots, n$
- (Assumption 4) _Positivity (or overlap)_: $a_i \in \mathcal{A}
  \implies d(a_i, w_i) \in \mathcal{A}$ for all $w \in \mathcal{W}$, where
  $\mathcal{A}$ denotes the support of $A \mid W = w_i \quad \forall i = 1,
  \ldots n$

Semiparametric-Efficient Estimation
- Semiparametric-efficient estimation through solving the efficient influence
  function estimating equation wrt $\M$.
- Statistical target parameter:
  $\Psi(P_0) = \mathbb{E}_{P_0}{\overline{Q}(d(A, W), W)}$
- For which the efficient influence function (EIF) is
  \begin{equation*}
    D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
    \overline{Q}(d(a, w), w) - \Psi(P_0)
  \end{equation*}
- The auxiliary covariate $H(a,w)$ may be expressed
  \begin{equation*}
    H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)}
    {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w))
  \end{equation*}
- The auxiliary covariate simplifies when the treatment is in the limits
  (conditional on $W$) --- i.e., for $A_i \in (u(w) - \delta, u(w))$, then
  we have $H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1$.
- Need to explicitly remind the audience what $u(w)$ is again. It has only
  appeared once at this point, and only been mentioned in passing.

- __Asymptotic linearity:__
  \begin{equation*}
    \Psi(P_n^{\star}) - \Psi(P_0) = \frac{1}{n} \sum_{i = 1}^{n} D(P_0)(X_i) +
    o_P\left(\frac{1}{\sqrt{n}}\right)
  \end{equation*}
- Gaussian limiting distribution:
  \begin{equation*}
    \sqrt{n}(\Psi(P_n^{\star}) - \Psi(P_0)) \to N(0, Var(D(P_0)(O)))
  \end{equation*}
- Statistical inference:
  \begin{equation*}
    \text{Wald-type CI}: \Psi(P_n^{\star}) \pm z_{\alpha} \cdot
    \frac{\sigma_n}{\sqrt{n}},
  \end{equation*}
  where $\sigma_n^2$ is computed directly via
  $\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\cdot)(O_i)$.

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right),$ we have that
$\Psi_n - \Psi_0 = (P_n - P_0) \cdot D(P_0) + o_P
\left( \frac{1}{\sqrt{n}} \right),$ which, by a central limit theorem,
establishes a Gaussian limiting distribution for the estimator, with variance
$V(D(P_0))$, the variance of the efficient influence function
when $\Psi$ admits an asymptotically linear representation.

The above implies that $\Psi_n$ is a $\sqrt{n}$-consistent estimator of $\Psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals, where
$\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via
$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$

We obtain semiparametric-efficient estimation and robust inference in the
nonparametric model $\M$ by solving the efficient influence function.

1. If $D(\bar{Q}_n^*, g_n)$ converges to $D(P_0)$ in $L_2(P_0)$ norm.
2. The size of the class of functions $\bar{Q}_n^*$ and $g_n$ is bounded
   (technically, $\exists \mathcal{F}$ st
   $D(\bar{Q}_n^*, g_n) \in \mathcal{F}$ whp, where $\mathcal{F}$ is a
   Donsker class)

- Construct initial estimators $g_n$ of $g_0(A, W)$ and $Q_n$ of
  $\overline{Q}_0(A, W)$, perhaps using data-adaptive regression techniques.
- For each observation $i$, compute an estimate $H_n(a_i, w_i)$ of the
  auxiliary covariate $H(a_i,w_i)$.
- Estimate the parameter $\epsilon$ in the logistic regression model
  $$ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
  \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),$$
  or an alternative regression model incorporating weights.
- Compute TML estimator $\Psi_n$ of the target parameter, defining update
  $\overline{Q}_n^{\star}$ of the initial estimate
  $\overline{Q}_{n, \epsilon_n}$:
\begin{equation*}\label{tmle}
  \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
  \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
\end{equation*}

- We recommend using nonparametric methods for the initial estimators,
  as consistent estimation is necessary for efficiency of the estimator
  $\Psi_n$.
- Intuition for the submodel fluctuation?

#### Literature: @haneuse2013estimation

- \textit{Proposal:} Characterization of stochastic interventions as
  \textit{modified treatment policies} (MTPs).
- Assumption of \textit{piecewise smooth invertibility} allows for the
  intervention distribution of any MTP to be recovered:
  \begin{equation*}
    g_{0, \delta}(a \mid w) = \sum_{j = 1}^{J(w)} I_{\delta, j} \{h_j(a, w),
    w\} g_0\{h_j(a, w) \mid w\} h^{\prime}_j(a,w)
  \end{equation*}
- Such intervention policies account for the natural value of the
  intervention $A$ directly yet are interpretable as the imposition of an
  altered intervention mechanism.
- Identification conditions for assessing the parameter of interest under
  such interventions appear technically complex (at first).
- Shifts of the form $d(A,W)$ are considerably more interesting since
  these are realistic intervention policies.
- Example: consider an individual with an extremely high immune response
  but whose baseline covariates $W$ suggest we shift the response still
  higher. Such a shift may not be biologically plausible (impossible, even)
  but we cannot account for this if the shift is only a function of $W$.
- The authors build IPW, outcome regression, and non-iterative doubly
  robust estimators, as well as an approach based on MSMs.
- Piecewise smooth invertibility: This assumption ensures that we can
  use the change of variable formula when computing integrals over $A$ and
  it is useful to study the estimators that we propose in this paper.

#### Literature: @young2014identification

- Establishes equivalence between g-formula when proposed intervention
  depends on natural value and when it does not.
- This equivalence leads to a sufficient positivity condition for
  estimating the counterfactual mean under MTPs via the same statistical
  functional studied in \cite{diaz2012population}.
- Extends earlier identification results, providing a way to use the same
  statistical functional to assess $\mathbb{E} Y_{d(A,W)}$ or
  $\mathbb{E} Y_{d(W)}$.
- The authors also consider limits on implementing shifts $d(A,W)$, and
  address working in a longitudinal setting.


## Interpreting the Causal Effect of a Stochastic Intervention

TODO

## Evaluating the Causal Effect of a Stochastic Intervention

To start, let us load the packages we will use and set a seed for simulation:

```{r setup-shift, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(condensier)
library(sl3)
library(tmle3)
library(tmle3shift)
set.seed(429153)
```

We need to estimate two components of the likelihood in order to...

The first of these is the outcome regression, $\hat{Q}_n$, which is a simple
regression of the form $\mathbb{E}[Y \mid A, W]$. An estimate for such a
quantity may be constructed using the Super Learner algorithm:

```{r sl3_lrnrs-Qfit-shift, message=FALSE, warning=FALSE}
# learners used for conditional expectation regression
lrn_mean <- Lrnr_mean$new()
lrn_fglm <- Lrnr_glm_fast$new()
lrn_xgb <- Lrnr_xgboost$new(nrounds=200)
lrn_hal <- Lrnr_hal9001$new()
sl_lrn <- Lrnr_sl$new(
  learners = list(lrn_mean, lrn_fglm), #, lrn_xgb, lrn_hal),
  metalearner = Lrnr_nnls$new()
)
```

The second of these is an estimate of the treatment mechanism, $\hat{g}_n$,
i.e., the _propensity score_. In the case of a continuous intervention node
$A$, such a quantity takes the form $p(A \mid W)$, which is a conditional
density. Generally speaking, conditional density estimation is a challenging
problem that has received much attention in the literature (see, for example,
...). To perform conditional density estimation, we focus on approach... 

```{r l3_lrnrs-gfit-shift, message=FALSE, warning=FALSE}
# learners used for conditional density regression
lrn_mean_dens <- Lrnr_condensier$new(
  nbins = 20, bin_estimator = lrn_mean,
  bin_method = "dhist"
)
lrn_fglm_dens <- Lrnr_condensier$new(
  nbins = 10, bin_estimator = lrn_fglm,
  bin_method = "dhist"
)
lrn_xgb_dens <- Lrnr_condensier$new(
  nbins = 5, bin_estimator = lrn_xgb,
  bin_method = "dhist"
)
sl_lrn_dens <- Lrnr_sl$new(
  learners = list(lrn_mean_dens, lrn_fglm_dens, lrn_xgb_dens),
  metalearner = Lrnr_solnp_density$new()
)

# specify outcome and treatment regressions and create learner list
Q_learner <- sl_lrn
g_learner <- sl_lrn_dens
learner_list <- list(Y = Q_learner, A = g_learner)
```

The `learner_list` object above specifies the role that each of the ensemble
learners we have generated is to play in computing initial estimators to be
used in building a TMLE for the parameter of interest here. In particular, it
makes explicit the fact that our `Q_learner` is used in fitting the outcome
regression while our `g_learner` is used in estimating the treatment mechanism.

### Simulate Data

```{r sim_data, message=FALSE, warning=FALSE}
# simulate simple data for tmle-shift sketch
n_obs <- 1000 # number of observations
tx_mult <- 2 # multiplier for the effect of W = 1 on the treatment

## baseline covariates -- simple, binary
W <- replicate(2, rbinom(n_obs, 1, 0.5))

## create treatment based on baseline W
A <- rnorm(n_obs, mean = tx_mult * W, sd = 1)

## create outcome as a linear function of A, W + white noise
Y <- rbinom(n_obs, 1, prob = plogis(A + W))

# organize data and nodes for tmle3
data <- data.table(W, A, Y)
setnames(data, c("W1", "W2", "A", "Y"))
node_list <- list(W = c("W1", "W2"), A = "A", Y = "Y")
head(data)
```

The above composes our observed data structure $O = (W, A, Y)$. To formally
express this fact using the `tlverse` grammar introduced by the `tmle3` package,
we create a single data object and specify the functional relationships between
the nodes in the _directed acyclic graph_ (DAG) via _nonparametric structural
equation models_ (NPSEMs), reflected in the node list that we set up:

We now have an observed data structure (`data`) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle_shift`. We specify the argument `shift_val = 0.5` when initializing the
`tmle3_Spec` object to communicate that we're interested in a shift of $0.5$ on
the scale of the treatment $A$ -- that is, we specify $\delta = 0.5$ (note that
this is an arbitrarily chosen value for this example).

```{r spec_init-shift, message=FALSE, warning=FALSE}
# initialize a tmle specification
tmle_spec <- tmle_shift(shift_val = 0.5,
                        shift_fxn = shift_additive_bounded,
                        shift_fxn_inv = shift_additive_bounded_inv)
```

As seen above, the `tmle_shift` specification object (like all `tmle3_Spec`
objects) does _not_ store the data for our specific analysis of interest. Later,
we'll see that passing a data object directly to the `tmle3` wrapper function,
alongside the instantiated `tmle_spec`, will serve to construct a `tmle3_Task`
object internally (see the `tmle3` documentation for details).

### Targeted Estimation of Stochastic Interventions Effects

```{r fit_tmle-shift, message=FALSE, warning=FALSE, cache=FALSE}
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)
tmle_fit
```

The `print` method of the resultant `tmle_fit` object conveniently displays the
results from computing our TML estimator.

### Statistical Inference for Targeted Maximum Likelihood Estimates

Recall that the asymptotic distribution of TML estimators has been studied
thoroughly:
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(\bar{Q}_n^*, g_n) + R(\hat{P}^*, P_0),$$
which, provided the following two conditions:

1. If $D(\bar{Q}_n^*, g_n)$ converges to $D(P_0)$ in $L_2(P_0)$ norm, and
2. the size of the class of functions considered for estimation of $\bar{Q}_n^*$
   and $g_n$ is bounded (technically, $\exists \mathcal{F}$ st
   $D(\bar{Q}_n^*, g_n) \in \mathcal{F}$ *__whp__*, where $\mathcal{F}$ is a
   Donsker class),
readily admits the conclusion that
$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + R(\hat{P}^*, P_0)$.

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right),$ we have that
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}}
 \right),$$
which, by a central limit theorem, establishes a Gaussian limiting distribution
for the estimator:

$$\sqrt{n}(\psi_n - \psi) \to N(0, V(D(P_0))),$$
where $V(D(P_0))$ is the variance of the efficient influence curve (canonical
gradient) when $\psi$ admits an asymptotically linear representation.

The above implies that $\psi_n$ is a $\sqrt{n}$-consistent estimator of $\psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals in a
straightforward manner:

$$\psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}},$$
where $\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via the following

$$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$$

Having now re-examined these facts, let's simply examine the results of
computing our TML estimator:

---

## Extensions: Variable Importance Analysis with Stochastic Interventions

### Defining a grid of counterfactual interventions

In order to specify a _grid_ of shifts $\delta$ to be used in defining a set of
stochastic intervention policies in an _a priori_ manner, let us consider an
arbitrary scalar $\delta$ that defines a counterfactual outcome $\psi_n =
Q_n(d(A, W), W)$, where, for simplicity, let $d(A, W) = A + \delta$. A
simplified expression of the auxiliary covariate for the TMLE of $\psi$ is
$H_n = \frac{g^*(a \mid w)}{g(a \mid w)}$, where $g^*(a \mid w)$ defines the
treatment mechanism with the stochastic intervention implemented. Then, to
ascertain whether a given choice of the shift $\delta$ is admissable (in the
sense of avoiding violations of the positivity assumption), let there be a bound
$C(\delta) = \frac{g^*(a \mid w)}{g(a \mid w)} < M$, where $g^*(a \mid w)$ is a
function of $\delta$ in part, and $M$ is a potentially user-specified upper
bound of $C(\delta)$. Then, $C(\delta)$ is a measure of the influence of a given
observation (under a bound of the conditional densities), which provides a way
to limit the maximum influence of a given observation through a choice of the
shift $\delta$.

We formalize and extend the procedure to determine an acceptable set of values
for the shift $\delta$ in the sequel. Specifically, let there be a shift $d(A,
W) = A + \delta(A, W)$, where the shift $\delta(A, W)$ is defined as
\begin{equation}
  \delta(a, w) =
    \begin{cases}
      \delta, & \delta_{\text{min}}(a,w) \leq \delta \leq
        \delta_{\text{max}}(a,w) \\
      \delta_{\text{max}}(a,w), & \delta \geq \delta_{\text{max}}(a,w) \\
      \delta_{\text{min}}(a,w), & \delta \leq \delta_{\text{min}}(a,w) \\
    \end{cases},
\end{equation}
where $$\delta_{\text{max}}(a, w) = \text{argmax}_{\left\{\delta \geq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}$$ and
$$\delta_{\text{min}}(a, w) = \text{argmin}_{\left\{\delta \leq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}.$$

The above provides a strategy for implementing a shift at the level of a given
observation $(a_i, w_i)$, thereby allowing for all observations to be shifted to
an appropriate value -- whether $\delta_{\text{min}}$, $\delta$, or
$\delta_{\text{max}}$.

### Initializing `vimshift` through its `tmle3_Spec`

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle_shift`. We specify the argument `shift_grid = seq(-1, 1, by = 1)`
when initializing the `tmle3_Spec` object to communicate that we're interested
in assessing the mean counterfactual outcome over a grid of shifts `r seq(-1,
1, by = 1)` on the scale of the treatment $A$ (note that the numerical
choice of shift is an arbitrarily chosen set of values for this example).

```{r vim_spec_init, message=FALSE, warning=FALSE}
# what's the grid of shifts we wish to consider?
delta_grid <- seq(-1, 1, 1)

# initialize a tmle specification
tmle_spec <- tmle_vimshift_delta(shift_grid = delta_grid,
                                 max_shifted_ratio = 2)
```

As seen above, the `tmle_vimshift` specification object (like all `tmle3_Spec`
objects) does _not_ store the data for our specific analysis of interest. Later,
we'll see that passing a data object directly to the `tmle3` wrapper function,
alongside the instantiated `tmle_spec`, will serve to construct a `tmle3_Task`
object internally (see the `tmle3` documentation for details).

### Targeted Estimation of Stochastic Interventions Effects

One may walk through the step-by-step procedure for  fitting the TML estimator
of the mean counterfactual outcome under each shift in the grid, using the
machinery exposed by the [`tmle3` R package](https://tmle3.tlverse.org/).

One may invoke the `tmle3` wrapper function (a user-facing convenience utility)
to fit the series of TML estimators (one for each parameter defined by the grid
delta) in a single function call:

```{r fit_tmle_wrapper, message=FALSE, warning=FALSE, cache=FALSE}
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)
tmle_fit
```

_Remark_: The `print` method of the resultant `tmle_fit` object conveniently
displays the results from computing our TML estimator.

### Inference with Marginal Structural Models

In the directly preceding section, we consider estimating the mean
counterfactual outcome $\psi_n$ under several values of the intervention
$\delta$, taken from the aforementioned $\delta$-grid. We now turn our attention
to an approach for obtaining inference on a single summary measure of these
estimated quantities. In particular, we propose summarizing the estimates
$\psi_n$ through a marginal structural model (MSM), obtaining inference by way
of a hypothesis test on a parameter of this working MSM. For a data structure
$O = (W, A, Y)$, let $\psi_{\delta}(P_0)$ be the mean outcome under a shift
$\delta$ of the treatment, so that we have $\vec{\psi}_{\delta} =
(\psi_{\delta}: \delta)$ with corresponding estimators $\vec{\psi}_{n, \delta}
= (\psi_{n, \delta}: \delta)$. Further, let $\beta(\vec{\psi}_{\delta}) =
\phi((\psi_{\delta}: \delta))$.

For a given MSM $m_{\beta}(\delta)$, we have that
$$\beta_0 = \text{argmin}_{\beta} \sum_{\delta}(\psi_{\delta}(P_0) -
m_{\beta}(\delta))^2 h(\delta),$$
which is the solution to
$$u(\beta, (\psi_{\delta}: \delta)) = \sum_{\delta}h(\delta)
\left(\psi_{\delta}(P_0) - m_{\beta}(\delta) \right) \frac{d}{d\beta}
m_{\beta}(\delta) = 0.$$
This then leads to the following expansion
$$\beta(\vec{\psi}_n) - \beta(\vec{\psi}_0) \approx -\frac{d}{d\beta} u(\beta_0,
\vec{\psi}_0)^{-1} \frac{d}{d\psi} u(\beta_0, \psi_0)(\vec{\psi}_n -
\vec{\psi}_0),$$
where we have
$$\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta)
-\sum_{\delta} h(\delta) m_{\beta}(\delta) \frac{d^2}{d\beta^2}
m_{\beta}(\delta),$$
which, in the case of an MSM that is a linear model (since
$\frac{d^2}{d\beta^2} m_{\beta}(\delta) = 0$), reduces simply to
$$\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta),$$
and
$$\frac{d}{d\psi}u(\beta, \psi)(\psi_n - \psi_0) = \sum_{\delta} h(\delta)
\frac{d}{d\beta} m_{\beta}(\delta) (\psi_n - \psi_0)(\delta),$$
which we may write in terms of the efficient influence function (EIF) of $\psi$
by using the first order approximation $(\psi_n - \psi_0)(\delta) =
\frac{1}{n}\sum_{i = 1}^n \text{EIF}_{\psi_{\delta}}(O_i)$,
where $\text{EIF}_{\psi_{\delta}}$ is the efficient influence function (EIF) of
$\vec{\psi}$.

Now, say, $\vec{\psi} = (\psi(\delta): \delta)$ is d-dimensional, then we may
write the efficient influence function of the MSM parameter $\beta$ (assuming a
linear MSM) as follows
$$\text{EIF}_{\beta}(O) = \left(\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta) \frac{d}{d\beta} m_{\beta}(\delta)^t \right)^{-1} \cdot
\sum_{\delta} h(\delta) \frac{d}{d\beta} m_{\beta}(\delta)
\text{EIF}_{\psi_{\delta}}(O),$$ where the first term is of dimension
$d \times d$ and the second term is of dimension $d \times 1$.

In an effort to generalize still further, consider the case where
$\psi_{\delta}(P_0) \in (0, 1)$ -- that is, $\psi_{\delta}(P_0)$ corresponds
to the probability of some event of interest. In such a case, it would be more
natural to consider a logistic MSM
$$m_{\beta}(\delta) = \frac{1}{1 + \exp(-f_{\beta}(\delta))},$$
where $f_{\beta}$ is taken to be linear in $\beta$ (e.g.,
$f_{\beta} = \beta_0 + \beta_1 \delta + \ldots$). In such a case, we have the
parameter of interest
$$\beta_0 = \text{argmax}_{\beta} \sum_{\delta} \left(\psi_{\delta}(P_0)
\text{log} m_{\beta}(\delta) + (1 - \psi_{\delta}(P_0))\log(1 -
m_{\beta}(\delta))\right)h(\delta),$$
where $\beta_0$ solves the following
$$
\sum_{\delta} h(\delta) \frac{d}{d\beta} f_{\beta}(\delta) (\psi_{\delta}(P_0)
- m_{\beta}(\delta)) = 0.$$

Inference from a working MSM is rather straightforward. To wit, the limiting
distribution for $m_{\beta}(\delta)$ may be expressed
$$\sqrt{n}(\beta_n - \beta_0) \to N(0, \Sigma),$$
where $\Sigma$ is the empirical covariance matrix of $\text{EIF}_{\beta}(O)$.

```{r msm_fit, message=FALSE, warning=FALSE}
tmle_fit$summary[4:5, ]
```

#### Directly Targeting the MSM Parameter $\beta$

Note that in the above, a working MSM is fit to the individual TML estimates of
the mean counterfactual outcome under a given value of the shift $\delta$ in the
supplied grid. The parameter of interest $\beta$ of the MSM is asymptotically
linear (and, in fact, a TML estimator) as a consequence of its construction from
individual TML estimators. In smaller samples, it may be prudent to perform a
TML estimation procedure that targets the parameter $\beta$ directly, as opposed
to constructing it from several independently targeted TML estimates. An
approach for constructing such an estimator is proposed in the sequel.

Suppose a simple working MSM $\mathbb{E}Y_{g^0_{\delta}} = \beta_0 + \beta_1
\delta$, then a TML estimator targeting $\beta_0$ and $\beta_1$ may be
constructed as
$$\overline{Q}_{n, \epsilon}(A,W) = \overline{Q}_n(A,W) + \epsilon (H_1(g),
H_2(g),$$ for all $\delta$, where $H_1(g)$ is the auxiliary covariate for
$\beta_0$ and $H_2(g)$ is the auxiliary covariate for $\beta_1$.

To construct a targeted maximum likelihood estimator that directly targets the
parameters of the working marginal structural model, we may use the
`tmle_vimshift_msm` Spec (instead of the `tmle_vimshift_delta` Spec that
appears above):

```{r vim_targeted_msm_fit, message=FALSE, warning=FALSE, cache=FALSE}
# initialize a tmle specification
tmle_msm_spec <- tmle_vimshift_msm(shift_grid = delta_grid,
                                   max_shifted_ratio = 2)

# fit the TML estimator and examine the results
tmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)
tmle_msm_fit
```

---

## Exercises

### Basics/Review

1. TODO

2. Set the `sl3` library of algorithms for the Super Learner

3. TODO

4. Describe two (equivalent) ways in which the causal effects of stochastic
   interventions may be interpreted.

### Using the Ideas

1. Choose a different variable of interest (e.g., TBD) and repeat the initial
   analysis we performed. That is, estimate the counterfactual mean under a
   shift of the new variable, after standardizing the chosen variable to have
   zero mean and unit variance.

2. TODO

3. TODO

4. What advantages, if any, are there to targeted directly the parameters of a
   marginal structural model?

### Advanced

1. How does the marginal structural model we used to summarize...

2. TODO

