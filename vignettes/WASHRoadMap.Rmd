---
title: "Statistical Roadmap and Data Intro"
author: "Alan Hubbard"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: refs.bib
editor_options: 
  chunk_output_type: console
---

## Learning Objectives
1. Introduce the data being used in workshp
2. Introduce the statistical roadmap on which the methods are based.

## The Data
The data come from the Effect of water quality, sanitation, hand washing, and
nutritional interventions on child development in rural
Bangladesh (WASH Benefits Bangladesh):
a cluster-randomised controlled trial (@luby2018effects). The study enrolled enrolled pregnant women in their first or second trimester
from rural villages of Gazipur, Kishoreganj, Mymensingh, and Tangail districts of central Bangladesh, with an average
of eight women per cluster. Groups of eight geographically adjacent clusters were block-randomised, using a random
number generator, into six intervention groups (all of which received weekly visits from a community health promoter
for the first 6 months and every 2 weeks for the next 18 months) and a double-sized control group (no intervention or
health promoter visit). The six intervention groups were: chlorinated drinking water; improved sanitation;
handwashing with soap; combined water, sanitation, and handwashing; improved nutrition through counselling and
provision of lipid-based nutrient supplements; and combined water, sanitation, handwashing, and nutrition. In the workshop, we concentrate child-growth (size for age) as the outcome.  This trial was registered with ClinicalTrials.gov, number NCT01590095.

```{r}
library(tidyverse)
library(tlverse)
## Read in data
dat <- read_csv("../washb_data/washb_data.csv") 
tibble(dat)
```

For the purposes of this workshop, we we start by treating the data as independent and identically distributed (i.i.d.) random draws from a very large target population.  We could, with available options,  account for the clustering of the data (within sampled geographic units), but avoiding these details for this workshop.

We have 2 variables and 1 variable set of interest.  Our outcome, $Y$, is *whz* -- weight For height Z-score, the treatment of interest, $A$, is *tr* -- randomized treatment group, and 
our adjustment variables, $W$, are *everything else*.  This results in our observed data structure as $i=1,...,n$ i.i.d. copies of $O_i = (W_i,A_i,Y_i)$. 

Modifications of our methods for biased samples, repeated measures, etc. are available. 

### The variables
```{r}
categ <- dat[,c(2,3,6,8,10,14:28)]
ncat <- dim(categ)[2]
vars.cat <- names(categ)
cont <- dat[,-c(2,3,6,8,10,14:28)]
ncont <- dim(cont)[2]
vars.cont <- names(cont)

# First do "continuous"" variables
var.out <- NULL
tab1.out <- NULL
for(i in 1:ncont) {
var.out <- c(var.out,vars.cont[i])
  x = as.numeric(unlist(cont[,i]))
  tab1.out <- rbind(tab1.out,c(mean(x,na.rm=T),sd(x,na.rm=T),NA,NA))
}

tab1.out <- data.frame(levels="",tab1.out)
names(tab1.out)<- c("levels","Mean","SD","Count","Row.Perc")
# Do categorical variables
for(i in 1:ncat) {
  x = unlist(categ[,i])
  tt <- table(x)
  levels <- names(tt)
  nt <- length(tt)
  tt <- as.vector(tt)
  out <- data.frame(levels=levels,Mean=rep(NA,nt),SD=rep(NA,nt),Count=tt,Row.Perc=tt/sum(tt))
names(out)<- c("levels","Mean","SD","Count","Row.Perc")
  
  var.out <- c(var.out,rep(vars.cat[i],nrow(out)))
  tab1.out <- rbind(tab1.out,out)
}

times <- rep("base",nrow(tab1.out))

tab2.out<-NULL
for(i in 1:ntd) {
var.out <- c(var.out,tm.dp[i])
  x = df.tab.td[,i]
  tab2.out <- rbind(tab2.out,c(mean(x,na.rm=T),sd(x,na.rm=T),NA,NA))
}

tab2.out <- data.frame(levels=rep("",nrow(tab2.out)),tab2.out)
names(tab2.out)<- c("levels","Mean","SD","Count","Row.Perc")

tab1.out <- rbind(tab1.out,tab2.out)
tab1.out <- data.frame(Variable=var.out,tab1.out)
tab1.out <- merge(tab1.out,final, by=1, all=T)
oo <- order(tab1.out$time.dep)
```

# Make table 1
```{r}
library(xtable)
# Change label from tabl
tab1=xtable::xtable(tab1.out[oo,c(7,2:6)], 
  caption = "Baseline variable values among those at risk of event (death/hemostasis) at first time interval (0.5 to 1.5 hour). Baseline variables and their means or counts and percentages. Total number of subjects enrolled is 680, and seven of those either died or achieved hemostasis by 0.5 hour after study began.", label = "table: 1", digits = 4,rownames=F)


```
print(xtable(out.wcgs,caption="ATE Results for WCGS",label="wcgsRes",digits=4),type="latex",file="wcgsRes.tex",caption.placement="top",include.rownames=T)
@
\include{wcgsRes}


## Motivating Example - Average Treatment Effect (ATE)

##$ Data Structure and Notation

TODO: modify for ATE
Consider $n$ observed units $O_1, \ldots, O_n$, where each random variable $O =
(W, A, Y)$ corresponds to a single observational unit. As discussed before, $W$
are baseline covariates, $A$ is a continuous-valued intervention, and $Y$ an
outcome of interest. We will consider a simple stochastic intervention that
considers counterfactual interventions defined through scalar shifts of the
observed/natural value of the intervention $A$. We'll start by considering a
simple additive shift $d(a,w) = a + \delta$, assuming support a.e. of
$P(A\midW)$. To ease violations of the positivity assumption, we may consider
extensions where $a \leq u(w) - \delta$ or $d(a, w) = a$ if $a \geq u(w) -
\delta$.

##$ Defining the Causal Effect of a Average Treatment Effect


##$ Interpreting the Causal Effect of a Stochastic Intervention

## Estimating an ATE with `tmle3`

* example code here
* how to extract/interpret the results


## `tmle3` Specifications

* wraps up all the details of the estimation strategy into a cohesive spec
* a function to search for these (similar to sl3_list_learners)
* example of applying a different spec

## Customizing `tmle3`

* A new set of parameters (which set)
* How much can we simplify this process

## Using the Delta method

* smooth transform of a parameter (or parameters)
* lay out math here

## Summary

* Do we want a summary section

## Exercises

### Introductory

1. Change the learner list

2. Estimate ATE for another intervention

3. 

4. ...

5. ...

### Intermediate

1. Estimate PAF 

2. Add known likelihood for g


### Advanced

1. ...

2. ...

## References

