<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown  and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/acic2019-workshop/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/acic2019-workshop" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook accompanying a full-day short-course on applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Mark van der Laan, Alan Hubbard, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="tmle3-targeted-learning-framework.html">
<link rel="next" href="stochastic-treatment-regimes.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-workshop"><i class="fa fa-check"></i>About this workshop</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-instructors"><i class="fa fa-check"></i>About the instructors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mark-van-der-laan"><i class="fa fa-check"></i>Mark van der Laan</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alan-hubbard"><i class="fa fa-check"></i>Alan Hubbard</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#jeremy-coyle"><i class="fa fa-check"></i>Jeremy Coyle</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nima-hejazi"><i class="fa fa-check"></i>Nima Hejazi</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i>Ivana Malenica</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rachael-phillips"><i class="fa fa-check"></i>Rachael Phillips</a></li>
</ul></li>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#preface-1"><i class="fa fa-check"></i><b>0.1</b> Preface</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Welcome to the <code>tlverse</code></a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#what-is-the-tlverse"><i class="fa fa-check"></i><b>1.2</b> What is the <code>tlverse</code>?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#tlverse-components"><i class="fa fa-check"></i><b>1.3</b> <code>tlverse</code> components</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#installation"><i class="fa fa-check"></i><b>1.4</b> Installation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#example-data---wash-benefits"><i class="fa fa-check"></i><b>1.5</b> Example Data - WASH Benefits</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html"><i class="fa fa-check"></i><b>2</b> Modern Super (Machine) Learning with <code>sl3</code></a><ul>
<li class="chapter" data-level="2.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#background"><i class="fa fa-check"></i><b>2.2</b> Background</a></li>
<li class="chapter" data-level="2.3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#modern-super-machine-learning-with-sl3-1"><i class="fa fa-check"></i><b>2.3</b> Modern Super (Machine) Learning with <code>sl3</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#basic-implementation"><i class="fa fa-check"></i><b>2.3.1</b> Basic Implementation</a></li>
<li class="chapter" data-level="2.3.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#extensions"><i class="fa fa-check"></i><b>2.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#exercise"><i class="fa fa-check"></i><b>2.4</b> Exercise</a></li>
<li class="chapter" data-level="2.5" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#appendix-more-advanced-extensions-of-sl3"><i class="fa fa-check"></i><b>2.5</b> Appendix: More advanced extensions of <code>sl3</code></a><ul>
<li class="chapter" data-level="2.5.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#variable-importance"><i class="fa fa-check"></i><b>2.5.1</b> Variable importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html"><i class="fa fa-check"></i><b>3</b> <code>tmle3</code> – Targeted Learning Framework</a><ul>
<li class="chapter" data-level="3.1" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#example-tmle3-for-ate"><i class="fa fa-check"></i><b>3.2</b> Example: <code>tmle3</code> for ATE</a><ul>
<li class="chapter" data-level="3.2.1" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#load-the-data"><i class="fa fa-check"></i><b>3.2.1</b> Load the Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#define-the-variable-roles"><i class="fa fa-check"></i><b>3.2.2</b> Define the variable roles</a></li>
<li class="chapter" data-level="3.2.3" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#handle-missingness"><i class="fa fa-check"></i><b>3.2.3</b> Handle Missingness</a></li>
<li class="chapter" data-level="3.2.4" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#create-a-spec-object"><i class="fa fa-check"></i><b>3.2.4</b> Create a “Spec” Object</a></li>
<li class="chapter" data-level="3.2.5" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#define-the-learners"><i class="fa fa-check"></i><b>3.2.5</b> Define the learners</a></li>
<li class="chapter" data-level="3.2.6" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#fit-the-tmle"><i class="fa fa-check"></i><b>3.2.6</b> Fit the TMLE</a></li>
<li class="chapter" data-level="3.2.7" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#evaluate-the-estimates"><i class="fa fa-check"></i><b>3.2.7</b> Evaluate the Estimates</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#tmle3-components"><i class="fa fa-check"></i><b>3.3</b> <code>tmle3</code> Components</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#tmle3_task"><i class="fa fa-check"></i><b>3.3.1</b> <code>tmle3_task</code></a></li>
<li class="chapter" data-level="3.3.2" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#initial-likelihood"><i class="fa fa-check"></i><b>3.3.2</b> Initial Likelihood</a></li>
<li class="chapter" data-level="3.3.3" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#targeted-likelihood-updater"><i class="fa fa-check"></i><b>3.3.3</b> Targeted Likelihood (updater)</a></li>
<li class="chapter" data-level="3.3.4" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#parameter-mapping"><i class="fa fa-check"></i><b>3.3.4</b> Parameter Mapping</a></li>
<li class="chapter" data-level="3.3.5" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#putting-it-all-together"><i class="fa fa-check"></i><b>3.3.5</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#fitting-tmle3-with-multiple-parameters"><i class="fa fa-check"></i><b>3.4</b> Fitting <code>tmle3</code> with multiple parameters</a><ul>
<li class="chapter" data-level="3.4.1" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#delta-method"><i class="fa fa-check"></i><b>3.4.1</b> Delta Method</a></li>
<li class="chapter" data-level="3.4.2" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#fit"><i class="fa fa-check"></i><b>3.4.2</b> Fit</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tmle3-targeted-learning-framework.html"><a href="tmle3-targeted-learning-framework.html#summary"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>4</b> Optimal Individualized Treatment Regimes</a><ul>
<li class="chapter" data-level="4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-objectives-3"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#introduction-to-optimal-individualized-interventions"><i class="fa fa-check"></i><b>4.2</b> Introduction to Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#data-structure-and-notation"><i class="fa fa-check"></i><b>4.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="4.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#defining-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>4.4</b> Defining the Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="4.4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment"><i class="fa fa-check"></i><b>4.4.1</b> Binary treatment</a></li>
<li class="chapter" data-level="4.4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment"><i class="fa fa-check"></i><b>4.4.2</b> Categorical treatment</a></li>
<li class="chapter" data-level="4.4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#why-cv-tmle"><i class="fa fa-check"></i><b>4.4.3</b> Why CV-TMLE?</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interpreting-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>4.5</b> Interpreting the Causal Effect of an Optimal Individualized Intervention</a></li>
<li class="chapter" data-level="4.6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-categorical-treatment"><i class="fa fa-check"></i><b>4.6</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with Categorical Treatment</a><ul>
<li class="chapter" data-level="4.6.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data"><i class="fa fa-check"></i><b>4.6.1</b> Simulated Data</a></li>
<li class="chapter" data-level="4.6.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3"><i class="fa fa-check"></i><b>4.6.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="4.6.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects"><i class="fa fa-check"></i><b>4.6.3</b> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
<li class="chapter" data-level="4.6.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-the-mean-outcome-under-the-optimal-rule-with-q-learning"><i class="fa fa-check"></i><b>4.6.4</b> Learning the Mean Outcome under the Optimal Rule with Q-learning</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-binary-treatment"><i class="fa fa-check"></i><b>4.7</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with Binary Treatment</a><ul>
<li class="chapter" data-level="4.7.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-1"><i class="fa fa-check"></i><b>4.7.1</b> Simulated Data</a></li>
<li class="chapter" data-level="4.7.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-1"><i class="fa fa-check"></i><b>4.7.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1"><i class="fa fa-check"></i><b>4.7.3</b> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
<li class="chapter" data-level="4.7.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extension-simpler-rules"><i class="fa fa-check"></i><b>4.7.4</b> Extension: Simpler Rules</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-the-mean-outcome-under-the-optimal-rule-with-q-learning-1"><i class="fa fa-check"></i><b>4.8</b> Learning the Mean Outcome under the Optimal Rule with Q-learning</a></li>
<li class="chapter" data-level="4.9" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-analysis-with-optimal-individualized-interventions"><i class="fa fa-check"></i><b>4.9</b> Variable Importance Analysis with Optimal Individualized Interventions</a><ul>
<li class="chapter" data-level="4.9.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-2"><i class="fa fa-check"></i><b>4.9.1</b> Simulated Data</a></li>
<li class="chapter" data-level="4.9.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-2"><i class="fa fa-check"></i><b>4.9.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="4.9.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-using-targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects"><i class="fa fa-check"></i><b>4.9.3</b> Variable Importance using Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
<li class="chapter" data-level="4.9.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-using-q-learning"><i class="fa fa-check"></i><b>4.9.4</b> Variable Importance using Q-learning</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercises"><i class="fa fa-check"></i><b>4.10</b> Exercises</a><ul>
<li class="chapter" data-level="4.10.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#basicsreview"><i class="fa fa-check"></i><b>4.10.1</b> Basics/Review</a></li>
<li class="chapter" data-level="4.10.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#using-the-ideas"><i class="fa fa-check"></i><b>4.10.2</b> Using the Ideas</a></li>
<li class="chapter" data-level="4.10.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#advanced"><i class="fa fa-check"></i><b>4.10.3</b> Advanced</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>5</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="5.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#learning-objectives-4"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#introduction-to-stochastic-interventions"><i class="fa fa-check"></i><b>5.2</b> Introduction to Stochastic Interventions</a></li>
<li class="chapter" data-level="5.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#data-structure-and-notation-1"><i class="fa fa-check"></i><b>5.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="5.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.4</b> Defining the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="5.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#interpreting-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.5</b> Interpreting the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="5.6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#evaluating-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>5.6</b> Evaluating the Causal Effect of a Stochastic Intervention</a><ul>
<li class="chapter" data-level="5.6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#simulate-data"><i class="fa fa-check"></i><b>5.6.1</b> Simulate Data</a></li>
<li class="chapter" data-level="5.6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>5.6.2</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="5.6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>5.6.3</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>5.7</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>5.7.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="5.7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>5.7.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="5.7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects-1"><i class="fa fa-check"></i><b>5.7.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="5.7.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>5.7.4</b> Inference with Marginal Structural Models</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises-1"><i class="fa fa-check"></i><b>5.8</b> Exercises</a><ul>
<li class="chapter" data-level="5.8.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#basicsreview-1"><i class="fa fa-check"></i><b>5.8.1</b> Basics/Review</a></li>
<li class="chapter" data-level="5.8.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#using-the-ideas-1"><i class="fa fa-check"></i><b>5.8.2</b> Using the Ideas</a></li>
<li class="chapter" data-level="5.8.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#advanced-1"><i class="fa fa-check"></i><b>5.8.3</b> Advanced</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimal-individualized-treatment-regimes" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Optimal Individualized Treatment Regimes</h1>
<p><em>Ivana Malenica, Jeremy Coyle, Mark van der Laan</em></p>
<p>Updated: 2019-04-01</p>
<div id="learning-objectives-3" class="section level2">
<h2><span class="header-section-number">4.1</span> Learning Objectives</h2>
<!--- appears as "X.1: Learning Objectives" in the book, where X is the chapter
corresponding to optimal interventions -->
<ol style="list-style-type: decimal">
<li>Understand and describe the essential properties of dynamic and optimal
individualized treatment regimes.</li>
<li>How may this formalism be used to define causal effects? How do dynamic
treatment regimes differ from static and stochastic regimes?</li>
<li>Understand and describe the benefits and challenges associated with using
optimal individualized treatment regimes in practice.</li>
<li>Use the <code>tmle3mopttx</code> R package to successfully estimate the causal effects
of an optimal individualized treatment on simulated data.</li>
<li>Understand and describe how variable importance measures may be defined in
terms of optimal individualized treatment interventions.</li>
<li>Perform hands-on, real-world data analysis to assess the causal effect of
optimal individualized treatment, successfully describing what
may be learned from the data based on the inferential properties of targeted
minimum loss-based estimators.</li>
</ol>
</div>
<div id="introduction-to-optimal-individualized-interventions" class="section level2">
<h2><span class="header-section-number">4.2</span> Introduction to Optimal Individualized Interventions</h2>
<p>The aim of precision medicine is to allow for patient specific interventions.
In the case of categorical treatment, one opts to administer the intervention to
individuals who will benefit from it, instead of assigning treatment on a population level.
For example, Abacavir and Tenofovir are commonly prescribed as part of the antiretroviral therapy to Human Immunodeficiency Virus (HIV) patients. However, not all individuals benefit from the two medications
equally. In particular, patients with renal dysfunction might further deteriorate if
prescribed Tenofovir, due to the high nephrotoxicity caused by the medication.
While Tenofovir is still highly effective treatment option for HIV patients, in order to maximize the
patient’s well-being, it would be beneficial to prescribe Tenofovir only to individuals
with healthy kidney function.</p>
<p>This motivates a different type of intervention, as opposed to the static exposures we
might be used to. In particular, in this chapter we learn about dynamic or individualized
interventions that tailor the treatment decision based on the collected covariates. In particular,
dynamic treatments represent interventions that at each treatment-decision stage are allowed
to respond to the currently available treatment and covariate history.
In the statistics community such a treatment strategy is termed
</p>
<p>The problem of estimating the optimal individualized treatment has received much attention
in the statistics literature over the years, especially with the advancement of
precision medicine; see <span class="citation">Murphy (<a href="#ref-murphy2003">2003</a>)</span>, <span class="citation">Robins (<a href="#ref-robins2004">2004</a>)</span>, <span class="citation">(“Temporary,” <a href="#ref-moodie2013">n.d.</a>)</span> and <span class="citation">(<span class="citeproc-not-found" data-reference-id="robins2014"><strong>???</strong></span>)</span> to name a few.
However, much of the early work depends on parametric assumptions. As such, even in a randomized trial,
the statistical inference for the optimal individualized treatment relies on assumptions that
are generally believed to be false, and can lead to biased results.</p>
<p>In this chapter, we consider estimation of the mean outcome under the optimal individualized treatment
where the candidate rules are restricted to depend only on user-supplied subset of the baseline covariates.
The estimation problem is addressed in a statistical model for the data distribution that is nonparametric,
and at most places restrictions on the probability of a patient receiving treatment given covariates (as in a
randomized trial). As
such, we don’t need to make any assumptions about the relationship of the outcome with the treatment and
covariates, or the relationship between the treatment and covariates. Further, we provide a Targeted
Maximum Likelihood Estimator for the mean under the optimal individualized treatment that allows us to
generate valid inference for our parameter, without having any parametric assumptions.
For a technical presentation of
the algorithm, the interested reader is invited to further consult <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span> and <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span>.
For additional background on Targeted Learning, please consider consulting <span class="citation">van der Laan and Rose (<a href="#ref-vdl2011targeted">2011</a>)</span> and
<span class="citation">van der Laan and Rose (<a href="#ref-vdl2018targeted">2018</a>)</span>.</p>
<hr />
</div>
<div id="data-structure-and-notation" class="section level2">
<h2><span class="header-section-number">4.3</span> Data Structure and Notation</h2>
<p>Suppose we observe <span class="math inline">\(n\)</span> independent and identically distributed observations of the form <span class="math inline">\(O=(W,A,Y) \sim P_0\)</span>. We denote <span class="math inline">\(A\)</span> as categorical treatment, and <span class="math inline">\(Y\)</span> as the final outcome. In particular, we define <span class="math inline">\(A \in \mathcal{A}\)</span> where <span class="math inline">\(\mathcal{A} \equiv \{a_1, \cdots, a_{n_A} \}\)</span> and <span class="math inline">\(n_A = |\mathcal{A}|\)</span>, with <span class="math inline">\(n_A\)</span> denoting the number of categories (possibly only two, for a binary setup). Note that we treat <span class="math inline">\(W\)</span> as vector-valued, representing all of our collected baseline covariates. Therefore, for a single random individual <span class="math inline">\(i\)</span>, we have that their observed data is <span class="math inline">\(O_i\)</span>: with corresponding baseline covariates <span class="math inline">\(W_i\)</span>, treatment <span class="math inline">\(A_i\)</span>, and final outcome <span class="math inline">\(Y_i\)</span>. We say that <span class="math inline">\(O \sim P_0\)</span>, or that all data was drawn from some probability distribution <span class="math inline">\(P_0\)</span>. We emphasize that we make no assumptions about the distribution of <span class="math inline">\(P_0\)</span>, so that <span class="math inline">\(P_0 \in \mathcal{M}\)</span>, where <span class="math inline">\(\mathcal{M}\)</span> is the fully nonparametric model. As previously mentioned, this means that we make no assumptions on the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span>, but might make assumptions regarding the relationship of <span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span>, as is the case in a randomized trial. We can break the data generating distribution <span class="math inline">\(P_0\)</span> into the following parts by time ordering:</p>
<p><span class="math display">\[P_0(O) = P_0(Y|A,W)P_0(A|W)P_0(W) = Q_{Y,0}(Y|A,W)g_0(A|W)Q_{W,0}(W)\]</span></p>
<p>where <span class="math inline">\(P_0(Y|A,W)=Q_{Y,0}(Y|A,W)\)</span>, <span class="math inline">\(P_0(A|W)=g_0(A|W)\)</span> and <span class="math inline">\(P_0(W)=Q_{W,0}(W)\)</span>. For notational simplicity, we also define <span class="math inline">\(\bar{Q}_{Y,0}(A,W) \equiv E_0[Y|A,W]\)</span>. In addition, we define <span class="math inline">\(V\)</span> as <span class="math inline">\(V \in W\)</span>, denoting a subset of the baseline covariates the optimal individualized rule will depend on. Note that <span class="math inline">\(V\)</span> could be all of <span class="math inline">\(W\)</span>, or an empty set, depending on the subject matter knowledge. In particular, a researcher might want to consider known effect modifiers available at the time of treatment decision as possible <span class="math inline">\(V\)</span> covariates.</p>
<p>We can assume a nonparametric structural equation model (NPSEM) to describe
generation of <span class="math inline">\(O\)</span> <span class="citation">Pearl (<a href="#ref-pearl2009causality">2009</a>)</span>. Specifically, we have that:
<span class="math display">\[\begin{align*}
  W &amp;= f_W(U_W) \\ A &amp;= f_A(W, U_A) \\ Y &amp;= f_Y(A, W, U_Y)
\end{align*}\]</span></p>
<p>In particular, NPSEM parameterizes <span class="math inline">\(P_0(O)\)</span> in terms of the distribution of random variables
<span class="math inline">\(O\)</span> and <span class="math inline">\(U\)</span>, where <span class="math inline">\(U=(U_W,U_A,U_Y)\)</span> are the exogenous random variables. Note that<br />
<span class="math inline">\(f_W\)</span>, <span class="math inline">\(f_A\)</span>, <span class="math inline">\(f_Y\)</span> are deterministic unspecified or partially specified functions.</p>
</div>
<div id="defining-the-causal-effect-of-an-optimal-individualized-intervention" class="section level2">
<h2><span class="header-section-number">4.4</span> Defining the Causal Effect of an Optimal Individualized Intervention</h2>
<p>Many methods for learning an optimal rule from data have been developed . In this chapter, we focus on the methods developed in <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> and <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>. Note however, that <code>tmle3mopttx</code> also supports the widely used Q-learning approach, where the optimal individualized rule is based on the initial estimate of <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> <span class="citation">Sutton, Barto, and others (<a href="#ref-Sutton1998">1998</a>)</span>.</p>
<p>In particular, we follow the methodology outlined in <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> and <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>, where we learn the optimal ITR using Super Learner <span class="citation">van der Laan, Polley, and Hubbard (<a href="#ref-vdl2007super">2007</a>)</span>, and estimate its value using the cross-validated Targeted Minimum Loss-based Estimation (CV-TMLE) <span class="citation">Zheng and van der Laan (<a href="#ref-cvtmle2010">2010</a>)</span>. Luedtke and van der Laan present three different approaches for learning the optimal rule- namely:</p>
<ol style="list-style-type: decimal">
<li><p>Super Learning the Blip Function</p></li>
<li><p>Super Learning the Weighted Classification Problem</p></li>
<li><p>Joint Super Learner of the Blip and Weighted Classification Problem</p></li>
</ol>
<p>The package <code>tmle3mopttx</code> relies on using the Super Learner to estimate the blip function, as it easily extends to more general categorical treatment. With that in mind, the loss function utilized for learning the optimal individualized rule corresponds to conditional mean type losses.</p>
<p>In great generality, we first need to estimate the true individual treatment regime, which corresponds to dynamic treatment rule (<span class="math inline">\(d(V)\)</span>) that takes a subset of covariates <span class="math inline">\(V \in W\)</span> and assigns treatment to each individual based on their observed covariates <span class="math inline">\(v\)</span>. We can define counterfactuals <span class="math inline">\(Y_{d(V)}\)</span> by modifying the NPSEM such that <span class="math inline">\(A=d(V)\)</span>, and denote the distribution of the counterfactual quantities as <span class="math inline">\(P_{0,d}(O)\)</span>. We are mostly interested in the value of such an individualized rule: <span class="math display">\[E_0[Y_{d(V)}] = E_{0,W}[\bar{Q}_{Y,0}(A=d(V),W)]\]</span> which, under typical causal assumptions, can be interpreted as the mean outcome if (possibly contrary to fact), treatment was assigned according to the rule.
The optimal rule is the rule with the maximal value: <span class="math display">\[d_{opt} \equiv \text{argmax}_{d \in \mathcal{D}} E_0[Y_{d(V)}] \]</span>
where <span class="math inline">\(\mathcal{D}\)</span> represents the set of possible rules, <span class="math inline">\(d\)</span>. We note that, in case the problem in hand requires minimizing the mean of an outcome, our optimal individualized rule will be the rule with the minimal value instead.</p>
<p>Under causal assumptions, we can identify <span class="math inline">\(P_{0,d}(O)\)</span> with observed data using the G-computation formula:</p>
<p><span class="math display">\[P_{0,d_{opt}}(O) = Q_{Y,0}(Y|A=d_{opt}(V),W)g_0(A=d_{opt}(V)|W)Q_{W,0}(W)\]</span></p>
<div id="binary-treatment" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Binary treatment</h3>
<p>How do we estimate the optimal individualized rule? In the case of a binary treatment, a key quantity for optimal ITR is the blip function. In particular, one can show that any optimal ITR assigns treatment to individuals falling in strata in which the stratum specific average treatment effect, the blip function, is positive and does not assign treatment to individuals for which this quantity is negative. Therefore for a binary treatment, we define a blip function as: <span class="math display">\[\bar{Q}_0(V) \equiv E_0[Y_1-Y_0|V] \equiv E_0[\bar{Q}_{Y,0}(1,W) - \bar{Q}_{Y,0}(0,W) | V], \]</span>
the average treatment effect within a stratum of <span class="math inline">\(V\)</span>. The note that the optimal individualized rule can now be derived as <span class="math inline">\(d_{opt}(V) = I(\bar{Q}_{0}(V) &gt; 0)\)</span>.</p>
<p>In particular, we will:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> and <span class="math inline">\(g_0(A|W)\)</span> using <code>sl3</code>. We denote such estimates as <span class="math inline">\(\bar{Q}_{Y,n}(A,W)\)</span> and <span class="math inline">\(g_n(A|W)\)</span>.</p></li>
<li><p>Apply the doubly robust Augmented-Inverse Probability Weighted (A-IPW) transform to our outcome, where we define:</p></li>
</ol>
<p><span class="math display">\[D_{\bar{Q}_Y,g,a}(O) \equiv \frac{I(A=a)}{g(A|W)} (Y-\bar{Q}_Y(A,W)) + \bar{Q}_Y(A=a,W)\]</span></p>
<p>note that under the randomization and positivity assumptions we have that <span class="math inline">\(E[D_{\bar{Q}_Y,g,a}(O) | V] = E[Y_a |V]\)</span>. Also, we emphasize its double robust nature- consistency of <span class="math inline">\(E[Y_a |V]\)</span> will depend on correct estimation of either <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> or <span class="math inline">\(g_0(A|W)\)</span>. As such, in a randomized trial, we are guaranteed a consistent estimate of <span class="math inline">\(E[Y_a |V]\)</span> even if we get <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> wrong!</p>
<p>Using this transform, we can define the following contrast:
<span class="math inline">\(D_{\bar{Q}_Y,g}(O) = D_{\bar{Q}_Y,g,a=1}(O) - D_{\bar{Q}_Y,g,a=0}(O)\)</span></p>
<p>We estimate the blip function, <span class="math inline">\(\bar{Q}_{0,a}(V)\)</span>, by regressing <span class="math inline">\(D_{\bar{Q}_Y,g}(O)\)</span> on <span class="math inline">\(V\)</span> using the specified <code>sl3</code> library of learners and an appropriate loss function.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Our estimated rule is <span class="math inline">\(d(V) = \text{argmax}_{a \in \mathcal{A}} \bar{Q}_{0,a}(V)\)</span>.</p></li>
<li><p>Obtain inference for the mean outcome under the estimated optimal rule using CV-TMLE.</p></li>
</ol>
</div>
<div id="categorical-treatment" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Categorical treatment</h3>
<p>In line with the approach considered for binary treatment, we extend the blip function to allow for categorical treatment. We denote such blip function extensions as , which are our new estimation targets in a categorical setting. We define pseudo-blips as vector valued entities where the output for a given <span class="math inline">\(V\)</span> is a vector of length equal to the number of treatment categories, <span class="math inline">\(n_A\)</span>. As such, we define it as:
<span class="math display">\[\bar{Q}_0^{pblip}(V) = \{\bar{Q}_{0,a}^{pblip}(V): a \in \mathcal{A} \}\]</span></p>
<p>We implement three different pseudo-blips in <code>tmle3mopttx</code>.</p>
<ol style="list-style-type: decimal">
<li><p> corresponds to choosing a reference category of treatment, and defining the blip for all other categories relative to the specified reference. Hence we have that: <span class="math display">\[\bar{Q}_{0,a}^{pblip-ref}(V) \equiv E_0(Y_a-Y_0|V)\]</span> where <span class="math inline">\(Y_0\)</span> is the specified reference category with <span class="math inline">\(A=0\)</span>. Note that, for the case of binary treatment, this strategy reduces to the approach described for the binary setup.</p></li>
<li><p> approach corresponds to defining the blip relative to the average of all categories. As such, we can define <span class="math inline">\(\bar{Q}_{0,a}^{pblip-avg}(V)\)</span> as:
<span class="math display">\[\bar{Q}_{0,a}^{pblip-avg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in \mathcal{A}} Y_{a|V)\]</span>
In the case where subject-matter knowledge regarding which reference category to use is not available, blip2 might be a viable option.</p></li>
<li><p> reflects an extension of Blip2, where the average is now a weighted average:
<span class="math display">\[\bar{Q}_{0,a}^{pblip-wavg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in \mathcal{A}} Y_{a} P(A=a|V)
|V)\]</span></p></li>
</ol>
<p>Just like in the binary case, pseudo-blips are estimated by regressing contrasts composed using the A-IPW transform on <span class="math inline">\(V\)</span>.
## Inference</p>
<p>In a randomized trial, statistical inference relies on the second-order difference between the estimator of the optimal
individualized treatment and the optimal individualized treatment itself to be asymptotically negligible. This is a reasonable condition if we consider rules that depend on small number of
covariates, or if we are willing to make smoothness assumptions. Alternatively, we can consider TMLEs
and statistical inference for data-adaptive target parameters defined in terms of an estimate
of the optimal individualized treatment. In particular, instead of trying to estimate the mean under the true
optimal individualized treatment, we aim to estimate the mean under the estimated optimal individualized treatment.
As such, we develop cross-validated TMLE approach that
provides asymptotic inference under minimal conditions for the mean under the estimate of the
optimal individualized treatment. In particular, considering the data adaptive parameter allows us
to avoid consistency and rate condition for the fitted optimal rule, as required for asymptotic
linearity of the TMLE of the mean under the actual, true optimal rule. Practically, the
estimated (data-adaptive) rule should be preferred, as this possibly sub-optimal rule is the one
implemented in the population.</p>
</div>
<div id="why-cv-tmle" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Why CV-TMLE?</h3>
<p>As discussed in <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>, CV-TMLE is necessary as the non-cross-validated TMLE is biased upward for the mean outcome under the rule- and therefore overly optimistic. More generally however, using CV-TMLE allows us more freedom in estimation and therefore greater data adaptivity, without sacrificing inference.</p>
</div>
</div>
<div id="interpreting-the-causal-effect-of-an-optimal-individualized-intervention" class="section level2">
<h2><span class="header-section-number">4.5</span> Interpreting the Causal Effect of an Optimal Individualized Intervention</h2>
<p>In summary, the mean outcome under the optimal individualized treatment is a counterfactual quantity of interest representing what the mean outcome would have been if everybody, contrary to the fact, received treatment that optimized their outcome. The optimal individualized treatment regime is a rule that optimizes the mean outcome under the
dynamic treatment, where the candidate rules are restricted to only respond to a user-supplied
subset of the baseline and intermediate covariates. In essence, our target parameter answers the key aim of precision medicine: allocating the available treatment by tailoring it to the individual characteristics of the patient, with the goal of optimizing the final outcome.</p>
</div>
<div id="evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-categorical-treatment" class="section level2">
<h2><span class="header-section-number">4.6</span> Evaluating the Causal Effect of an Optimal Individualized Intervention with Categorical Treatment</h2>
<p>Finally, we demonstrate how to evaluate the mean outcome under the optimal individualized treatment using <code>tmle3mopptx</code>.
To start, let’s load the packages we’ll use and set a seed:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(data.table)
<span class="kw">library</span>(sl3)
<span class="kw">library</span>(tmle3)
<span class="kw">library</span>(tmle3mopttx)
<span class="kw">library</span>(devtools)
<span class="kw">set.seed</span>(<span class="dv">111</span>)</code></pre>
<div id="simulated-data" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Simulated Data</h3>
<p>First, we load the simulated data. We will start with the more general setup where the treatment is a categorical variable; later in the chapter we will consider another data generating distribution where <span class="math inline">\(A\)</span> is binary. In this example, our data generating distribution is of the following form:</p>
<p><span class="math display">\[W \sim \mathcal{N}(\bf{0},I_{4 \times 4})\]</span>
<span class="math display">\[P(A=a|W) = \frac{1}{1+\exp^{(-0.8*W_a)}}\]</span></p>
<p><span class="math display">\[P(Y=1|A,W) = 0.5\text{logit}^{-1}[3I(A=1)(W_1-0.5) - 3I(A=2)(2W_2+0.5) + 3I(A=3)(3W_3-0.5)] +\text{logit}^{-1}(W_2W_3)\]</span></p>
<p>We can just load the data available as part of the package as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;data_cat&quot;</span>)</code></pre>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. Note that the
mean under the true optimal rule is <span class="math inline">\(\psi=0.625\)</span>.</p>
<p>To formally express this fact using the <code>tlverse</code> grammar introduced by the <code>tmle3</code> package,
we create a single data object and specify the functional relationships between
the nodes in the <em>directed acyclic graph</em> (DAG) via <em>nonparametric structural
equation models</em> (NPSEMs), reflected in the node list that we set up:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># organize data and nodes for tmle3</span>
data &lt;-<span class="st"> </span>data_cat
node_list &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W4&quot;</span>),
  <span class="dt">A =</span> <span class="st">&quot;A&quot;</span>,
  <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span>
)</code></pre>
<p>We now have an observed data structure (<code>data</code>) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.</p>
</div>
<div id="constructing-optimal-stacked-regressions-with-sl3" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Constructing Optimal Stacked Regressions with <code>sl3</code></h3>
<p>To easily incorporate ensemble machine learning into the estimation procedure,
we rely on the facilities provided in the <a href="https://sl3.tlverse.org"><code>sl3</code> R
package</a>.</p>
<p>Using the framework provided by the <a href="https://sl3.tlverse.org"><code>sl3</code> package</a>,
the nuisance parameters of the TML estimator may be fit with ensemble learning,
using the cross-validation framework of the Super Learner algorithm of
<span class="citation">van der Laan, Polley, and Hubbard (<a href="#ref-vdl2007super">2007</a>)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Initialize some of the learners.</span>
<span class="co">#Here we use xgboost with various parameters, glm, HAL and the mean.</span>
xgboost_<span class="dv">50</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">50</span>)
xgboost_<span class="dv">100</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">100</span>)
xgboost_<span class="dv">500</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">500</span>)
lrn1 &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()
lrn2&lt;-Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()
lrn3&lt;-Lrnr_hal9001<span class="op">$</span><span class="kw">new</span>()

<span class="co">#Define the Q learner, which is just a regular learner:</span>
Q_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(xgboost_<span class="dv">50</span>,xgboost_<span class="dv">100</span>,xgboost_<span class="dv">500</span>,lrn1,lrn2),
  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()
)

<span class="co">#Define the g learner, which is a multinomial learner:</span>
glib &lt;-<span class="st"> </span><span class="kw">list</span>(
  rf &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_randomForest),
  xgb &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_xgboost),
  glmnet &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glmnet),
  multinom_gf &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_independent_binomial, <span class="kw">make_learner</span>(Lrnr_glm_fast)),
  mean &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_mean)
)

<span class="co">#Specify the appropriate loss of the multinomial learner:</span>
mn_metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_solnp, <span class="dt">loss_function =</span> loss_loglik_multinomial, 
                               <span class="dt">learner_function =</span> metalearner_linear_multinomial)
g_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl, glib, mn_metalearner)

<span class="co">#Define the Blip learner, which is a multivariate learner:</span>
learners &lt;-<span class="st"> </span><span class="kw">list</span>(xgboost_<span class="dv">50</span>,xgboost_<span class="dv">100</span>,xgboost_<span class="dv">500</span>,lrn1,lrn2)
b_learner &lt;-<span class="st"> </span><span class="kw">create_mv_learners</span>(<span class="dt">learners =</span> learners)</code></pre>
<p>As seen above, we generate three different ensemble learners that must be fit, corresponding to the learners for the outcome regression, propensity score, and the blip function. Note that we need to estimate <span class="math inline">\(g_0(A|W)\)</span> for a categorical <span class="math inline">\(A\)</span>- therefore we use the multinomial Super Learner option available within the <code>sl3</code> package with learners that can address multi-class classification problems. In order to see which learners can be used to estimate <span class="math inline">\(g_0(A|W)\)</span> in <code>sl3</code>, we run the following:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#See which learners support multi-class classification:</span>
<span class="kw">sl3_list_learners</span>(<span class="kw">c</span>(<span class="st">&quot;categorical&quot;</span>))</code></pre>
<pre><code> [1] &quot;Lrnr_bartMachine&quot;          &quot;Lrnr_dbarts&quot;              
 [3] &quot;Lrnr_glmnet&quot;               &quot;Lrnr_grf&quot;                 
 [5] &quot;Lrnr_h2o_glm&quot;              &quot;Lrnr_h2o_grid&quot;            
 [7] &quot;Lrnr_independent_binomial&quot; &quot;Lrnr_mean&quot;                
 [9] &quot;Lrnr_multivariate&quot;         &quot;Lrnr_optim&quot;               
[11] &quot;Lrnr_randomForest&quot;         &quot;Lrnr_ranger&quot;              
[13] &quot;Lrnr_rpart&quot;                &quot;Lrnr_solnp&quot;               
[15] &quot;Lrnr_svm&quot;                  &quot;Lrnr_xgboost&quot;             </code></pre>
<p>Also note that since the corresponding blip will be vector valued, we will have a column for each additional level of treatment. As such, we need to use multivariate learners with the the helper function <code>create_mv_learners</code> that takes a list of initialized learners as input.</p>
<p>We make the above explicit with respect to standard notation by bundling the
ensemble learners into a list object below:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># specify outcome and treatment regressions and create learner list</span>
learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</code></pre>
<p>The <code>learner_list</code> object above specifies the role that each of the ensemble
learners we’ve generated is to play in computing initial estimators to be used
in building a TMLE for the parameter of interest. In particular, it makes
explicit the fact that our <code>Y</code> is used in fitting the outcome regression
while our <code>A</code> is used in fitting our treatment mechanism regression, and finally <code>B</code> is used in fitting the blip function.</p>
</div>
<div id="targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</h3>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest simply by calling <code>tmle3_mopttx_blip_revere</code>. We specify the argument <code>V = c(&quot;W1&quot;, &quot;W2&quot;, &quot;W3&quot;, &quot;W4&quot;)</code>
when initializing the <code>tmle3_Spec</code> object in order to communicate that we’re interested
in learning a rule dependent on <code>V</code> covariates. We also need to specify the type of
pseudo-blip we will use in this estimation problem, and finally the list of learners
used to estimate the blip function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(<span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W4&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>, 
                                      <span class="dt">b_learner =</span> learner_list<span class="op">$</span>B, <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>)</code></pre>
<p>As seen above, the <code>tmle3_mopttx_blip_revere</code> specification object (like all <code>tmle3_Spec</code>
objects) does <em>not</em> store the data for our specific analysis of interest. Later,
we’ll see that passing a data object directly to the <code>tmle3</code> wrapper function,
alongside the instantiated <code>tmle_spec</code>, will serve to construct a <code>tmle3_Task</code>
object internally.</p>
<p>In initializing the specification for the TMLE of our parameter of
interest, we have specified the set of covariates the rule depends on (<code>V</code>), the type of pseudo-blip to use (<code>type</code>), and the learners used for estimating the pseudo-blip. In addition, we need to specify whether we want to maximize the mean outcome under the rule (<code>maximize=TRUE</code>), and whether we want to estimate the rule under all the covariates <span class="math inline">\(V\)</span> provided by the user (<code>complex</code>). If FALSE, <code>tmle3mopttx</code> will instead consider all the possible rules under a smaller set of covariates including the static rules, and optimize the mean outcome over all the subsets of <span class="math inline">\(V\)</span>. As such, while the user might have provided a full set of collected covariates as input for <span class="math inline">\(V\)</span>, it is possible that the true rule only depends on a subset of the set provided by the user. In that case, our returned mean under the optimal individualized rule will be based on the smaller subset. We explore this important feature of <code>tmle3mopttx</code> in the later sections.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the TML estimator</span>
fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)
fit</code></pre>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est       se     lower     upper
1:  TSM E[Y_{A=NULL}] 0.4804623 0.5844556 0.025198 0.5350685 0.6338428
   psi_transformed lower_transformed upper_transformed
1:       0.5844556         0.5350685         0.6338428</code></pre>
<p>We can see that the estimate of <span class="math inline">\(psi_0\)</span> is <span class="math inline">\(0.58\)</span>, and that the confidence interval covers our true mean under the true optimal individualized treatment.</p>
</div>
<div id="learning-the-mean-outcome-under-the-optimal-rule-with-q-learning" class="section level3">
<h3><span class="header-section-number">4.6.4</span> Learning the Mean Outcome under the Optimal Rule with Q-learning</h3>
<p>Here we outline how to use <code>tmle3mopttx</code> package in order to estimate the mean under the ITR using Q-learning. As demonstrated in the previous sections, we first need to initialize a specification for the TMLE of our parameter of interest. As opposed to the previous section however, we will now use <code>tmle3_mopttx_Q</code> instead of <code>tmle3_mopttx_blip_revere</code> in order to indicate that we want to use Q-learning instead of TMLE.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec_Q &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_Q</span>(<span class="dt">maximize =</span> <span class="ot">TRUE</span>)

<span class="co"># Define data:</span>
tmle_task &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_tmle_task</span>(data, node_list)

<span class="co"># Define likelihood:</span>
initial_likelihood &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_initial_likelihood</span>(tmle_task, learner_list)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate the parameter: (broken by tmle3@1bc8f32)</span>
<span class="kw">Q_learning</span>(tmle_spec_Q, initial_likelihood, tmle_task)</code></pre>
</div>
</div>
<div id="evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-binary-treatment" class="section level2">
<h2><span class="header-section-number">4.7</span> Evaluating the Causal Effect of an Optimal Individualized Intervention with Binary Treatment</h2>
<p>Next, we consider how to evaluate the mean outcome under the optimal
individualized treatment when <span class="math inline">\(A\)</span> is binary. As outlined in previous sections,
our estimation procedure should rely on simple blip estimation corresponding to
<code>type=blip1</code> for binary treatment.</p>
<div id="simulated-data-1" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Simulated Data</h3>
<p>First, we load the simulated data. Here, our data generating distribution was
of the following form:</p>
<p><span class="math display">\[W \sim \mathcal{N}(\bf{0},I_{3 \times 3})\]</span>
<span class="math display">\[P(A=1|W) = \frac{1}{1+\exp^{(-0.8*W_1)}}\]</span></p>
<p><span class="math display">\[P(Y=1|A,W) = 0.5\text{logit}^{-1}[-5I(A=1)(W_1-0.5)+5I(A=0)(W_1-0.5)] +
0.5\text{logit}^{-1}(W_2W_3)\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;data_bin&quot;</span>)</code></pre>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. Note that the
mean under the true optimal rule is <span class="math inline">\(\psi=0.578\)</span> for this data generating
distribution.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># organize data and nodes for tmle3</span>
data &lt;-<span class="st"> </span>data_bin
node_list &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>),
  <span class="dt">A =</span> <span class="st">&quot;A&quot;</span>,
  <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span>
)</code></pre>
</div>
<div id="constructing-optimal-stacked-regressions-with-sl3-1" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Constructing Optimal Stacked Regressions with <code>sl3</code></h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define sl3 library and metalearners:</span>
xgboost_<span class="dv">50</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">50</span>)
xgboost_<span class="dv">100</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">100</span>)
xgboost_<span class="dv">500</span>&lt;-Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">500</span>)
lrn1 &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()
lrn2&lt;-Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()
lrn3&lt;-Lrnr_hal9001<span class="op">$</span><span class="kw">new</span>()

Q_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(xgboost_<span class="dv">50</span>,xgboost_<span class="dv">100</span>,xgboost_<span class="dv">500</span>,
                  lrn1,lrn2),
  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()
)

g_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(xgboost_<span class="dv">100</span>,lrn2),
  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()
)

b_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> <span class="kw">list</span>(xgboost_<span class="dv">50</span>,xgboost_<span class="dv">100</span>,xgboost_<span class="dv">500</span>,
                  lrn1,lrn2),
  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()
)</code></pre>
<p>As seen above, we generate three different ensemble learners that must be fit,
corresponding to the learners for the outcome regression, propensity score, and
the blip function. We make the above explicit with respect to standard notation
by bundling the ensemble learners into a list object below:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># specify outcome and treatment regressions and create learner list</span>
learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</code></pre>
</div>
<div id="targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(<span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip1&quot;</span>,
                                      <span class="dt">b_learner =</span> learner_list<span class="op">$</span>B,
                                      <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the TML estimator</span>
fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)
fit</code></pre>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est         se     lower     upper
1:  TSM E[Y_{A=NULL}] 0.4273824 0.5612336 0.02760086 0.5071369 0.6153303
   psi_transformed lower_transformed upper_transformed
1:       0.5612336         0.5071369         0.6153303</code></pre>
<p>We can see that the estimate of <span class="math inline">\(psi_0\)</span> is <span class="math inline">\(0.56\)</span>, and that the confidence
interval covers our true mean under the true optimal individualized treatment.</p>
</div>
<div id="extension-simpler-rules" class="section level3">
<h3><span class="header-section-number">4.7.4</span> Extension: Simpler Rules</h3>
<p>In order to not only consider the most ambitious fully V-optimal rule, we
define S-optimal rules as the optimal rule that considers all possible subsets
of <span class="math inline">\(V\)</span> covariates, with card(<span class="math inline">\(S\)</span>) <span class="math inline">\(\leq\)</span> card(<span class="math inline">\(V\)</span>) and <span class="math inline">\(\emptyset \in S\)</span>. This
allows us to consider sub-optimal rules that are easier to estimate and
potentially provide more realistic rules- as such, we allow for statistical
inference for the counterfactual mean outcome under the sub-optimal rule.
Within the <code>tmle3mopttx</code> paradigm, we just need to change the <code>complex</code>
parameter to <code>FALSE</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(<span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip1&quot;</span>,
                                      <span class="dt">b_learner =</span> learner_list<span class="op">$</span>B,
                                      <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">FALSE</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the TML estimator</span>
fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)
fit</code></pre>
<pre><code>A tmle3_Fit that took 1 step(s)
   type       param  init_est tmle_est         se     lower     upper
1:  TSM E[Y_{A=W3}] 0.4207201 0.551216 0.02731607 0.4976775 0.6047545
   psi_transformed lower_transformed upper_transformed
1:        0.551216         0.4976775         0.6047545</code></pre>
<p>Therefore even though the user specified all baseline covariates as the basis
for rule estimation, a simpler rule based on only <span class="math inline">\(W_3\)</span> is sufficient to
maximize the mean under the optimal individualized treatment.</p>
</div>
</div>
<div id="learning-the-mean-outcome-under-the-optimal-rule-with-q-learning-1" class="section level2">
<h2><span class="header-section-number">4.8</span> Learning the Mean Outcome under the Optimal Rule with Q-learning</h2>
<p>Alternatively, we could estimate the mean under the optimal individualized treatment using Q-learning.
The optimal rule can be learned through fitting the likelihood, and consequently estimating the
optimal rule under this fit of the likelihood .</p>
<p>Below we outline how to use <code>tmle3mopttx</code> package in order to estimate the mean under the ITR using Q-learning. As demonstrated in the previous sections, we first need to initialize a specification for the TMLE of our parameter of interest. As opposed to the previous section however, we will now use <code>tmle3_mopttx_Q</code> instead of <code>tmle3_mopttx_blip_revere</code> in order to indicate that we want to use Q-learning instead of TMLE.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec_Q &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_Q</span>(<span class="dt">maximize =</span> <span class="ot">TRUE</span>)

<span class="co"># Define data:</span>
tmle_task &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_tmle_task</span>(data, node_list)

<span class="co"># Define likelihood:</span>
initial_likelihood &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_initial_likelihood</span>(tmle_task, learner_list)

<span class="co"># Estimate the parameter:</span>
<span class="kw">Q_learning</span>(tmle_spec_Q, initial_likelihood, tmle_task)</code></pre>
<pre><code>[1] 0.3178034</code></pre>
</div>
<div id="variable-importance-analysis-with-optimal-individualized-interventions" class="section level2">
<h2><span class="header-section-number">4.9</span> Variable Importance Analysis with Optimal Individualized Interventions</h2>
<p>Suppose one wishes to assess the importance of each observed covariate, in
terms of maximizing (or minimizing) the population mean of an outcome under an
optimal individualized treatment regime. In particular, a covariate that
maximizes (or minimizes) the population mean outcome the most under an optimal
individualized treatment out of all other considered covariates under optimal
assignment might be considered “more important” for the outcome. To put it in
context, perhaps optimal allocation of treatment 1, denoted <span class="math inline">\(A_1\)</span>, results in a
larger mean outcome than optimal allocation of another treatment (<span class="math inline">\(A_2\)</span>).
Therefore, we would label <span class="math inline">\(A_1\)</span> as having a higher variable importance as
regard to maximizing the mean outcome under the optimal individualized
treatment.</p>
<div id="simulated-data-2" class="section level3">
<h3><span class="header-section-number">4.9.1</span> Simulated Data</h3>
<p>We consider the same data as described in the previous section, with our
treatment and outcome being binary variables. Note that here we have three
baseline covariates, hence our variable importance algorithm will consider
optimal allocation of all the available <span class="math inline">\(W\)</span>s and <span class="math inline">\(A\)</span>.</p>
</div>
<div id="constructing-optimal-stacked-regressions-with-sl3-2" class="section level3">
<h3><span class="header-section-number">4.9.2</span> Constructing Optimal Stacked Regressions with <code>sl3</code></h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Define sl3 library and metalearners:</span>
qlib &lt;-<span class="st"> </span><span class="kw">make_learner_stack</span>(
  <span class="st">&quot;Lrnr_mean&quot;</span>,
  <span class="st">&quot;Lrnr_glm_fast&quot;</span>
)

glib &lt;-<span class="st"> </span><span class="kw">make_learner_stack</span>(
  <span class="st">&quot;Lrnr_mean&quot;</span>,
  <span class="st">&quot;Lrnr_glmnet&quot;</span>,
  <span class="st">&quot;Lrnr_xgboost&quot;</span>
)

blib &lt;-<span class="st"> </span><span class="kw">make_learner_stack</span>(
  <span class="st">&quot;Lrnr_mean&quot;</span>,
  <span class="st">&quot;Lrnr_glm_fast&quot;</span>
)

metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_nnls)
mn_metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_solnp,
                               <span class="dt">loss_function =</span>
                                 loss_loglik_multinomial,
                               <span class="dt">learner_function =</span>
                                 metalearner_linear_multinomial)

Q_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl, qlib, metalearner)
g_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl, glib, mn_metalearner)
b_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl, blib, metalearner)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># specify outcome and treatment regressions and create learner list</span>
learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</code></pre>
</div>
<div id="variable-importance-using-targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects" class="section level3">
<h3><span class="header-section-number">4.9.3</span> Variable Importance using Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</h3>
<p>In the previous sections we have seen how to obtain a contrast between the
mean under the optimal individualized rule and the mean under the observed outcome for a
single covariate- we are now ready to run the variable importance analysis for all of our
observed covariates. In order to run the variable importance analysis, we first need
to initialize a specification for the TMLE of our parameter of interest as we have done
before. In addition, we need to specify the data and the corresponding list of nodes, as
well as the appropriate learners for the outcome regression, propensity score, and the blip
function. Finally, we need to specify whether we should adjust for all the other covariates
we are assessing variable importance for. Note that we are able to assess importance of only
categorical covariates- hence all continuous baseline covariates <span class="math inline">\(W\)</span> will not be included in the
variable importance loop, only <span class="math inline">\(A\)</span> terms. However, we will adjust for all <span class="math inline">\(W\)</span>s in our analysis, and
if <code>adjust_for_other_A=TRUE</code>, also for all <span class="math inline">\(A\)</span> covariates that are not treated as exposure in the
variable importance loop. For computational reasons, we set <code>adjust_for_other_A=FALSE</code> below.</p>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest (called a <code>tmle3_Spec</code> in the <code>tlverse</code> nomenclature) simply by calling
<code>tmle3_mopttx_vim</code>. First, we indicate the method used for learning the optimal individualized
treatment by specifying the <code>method</code> argument of <code>tmle3_mopttx_vim</code>. If <code>method=&quot;Q&quot;</code>, then
we will be using Q-learning for rule estimation, and we do not need to specify <code>V</code>, <code>type</code> and
<code>b_learner</code> arguments in the spec, since they are not important for Q-learning. However,
if <code>method=&quot;SL&quot;</code>, which corresponds to learning the optimal individualized treatment using the
above outlined methodology, then we need to specify the type of pseudo-blip we will use in this
estimation problem and the list of learners used to estimate the blip function. Finally, for
<code>method=&quot;SL&quot;</code> we also need to communicate that we’re interested in learning a rule dependent on
<code>V</code> covariates by specifying the <code>V</code> argument. For both <code>method=&quot;Q&quot;</code> and <code>method=&quot;SL&quot;</code>, we
need to indicate whether we want to maximize or minimize the mean under the optimal individualized
rule. Finally, we also need to specify whether the final comparison of the mean under the
optimal individualized rule and the mean under the observed outcome should be on the
multiplicative scale (risk ratio) or linear (similar to average treatment effect).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification</span>
tmle_spec &lt;-<span class="st">  </span><span class="kw">tmle3_mopttx_vim</span>(<span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>),
                               <span class="dt">type =</span> <span class="st">&quot;blip1&quot;</span>,
                               <span class="dt">b_learner =</span> learner_list<span class="op">$</span>B,
                               <span class="dt">contrast =</span> <span class="st">&quot;multiplicative&quot;</span>,
                               <span class="dt">maximize =</span> <span class="ot">FALSE</span>,
                               <span class="dt">method=</span><span class="st">&quot;SL&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the TML estimator</span>
vim_results &lt;-<span class="st"> </span><span class="kw">tmle3_vim</span>(tmle_spec, data, node_list, learner_list,
                         <span class="dt">adjust_for_other_A =</span> <span class="ot">FALSE</span>)
vim_results</code></pre>
<p>The final result of <code>tmle3_vim</code> with the <code>tmle3mopttx</code> spec is an ordered list
of mean outcomes under the optimal individualized treatment for all categorical
covariates in our dataset.</p>
</div>
<div id="variable-importance-using-q-learning" class="section level3">
<h3><span class="header-section-number">4.9.4</span> Variable Importance using Q-learning</h3>
<p>We can also perform variable importance with the optimal individualized
treatment estimated by Q-learning. In order to do that, we need to initialize
our <code>tmle3</code> spec with <code>method=&quot;Q&quot;</code>, then run as in the above section.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize a tmle specification:</span>
tmle_spec_Q &lt;-<span class="st">  </span><span class="kw">tmle3_mopttx_vim</span>(<span class="dt">contrast =</span> <span class="st">&quot;multiplicative&quot;</span>,
                               <span class="dt">maximize =</span> <span class="ot">FALSE</span>,
                               <span class="dt">method=</span><span class="st">&quot;Q&quot;</span>)

vim_results_Q &lt;-<span class="st"> </span><span class="kw">tmle3_vim</span>(tmle_spec_Q, data, <span class="dt">node_list=</span>node_list, learner_list,
                         <span class="dt">adjust_for_other_A =</span> <span class="ot">FALSE</span>)
vim_results_Q</code></pre>
<hr />
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">4.10</span> Exercises</h2>
<div id="basicsreview" class="section level3">
<h3><span class="header-section-number">4.10.1</span> Basics/Review</h3>
</div>
<div id="using-the-ideas" class="section level3">
<h3><span class="header-section-number">4.10.2</span> Using the Ideas</h3>
</div>
<div id="advanced" class="section level3">
<h3><span class="header-section-number">4.10.3</span> Advanced</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-luedtke2016super">
<p>Luedtke, A., and M. J van der Laan. 2016. “Super-Learning of an Optimal Dynamic Treatment Rule.” <em>International Journal of Biostatistics</em> 12 (1): 305–32.</p>
</div>
<div id="ref-murphy2003">
<p>Murphy, Susan A. 2003. “Optimal Dynamic Treatment Regimes.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 65 (2). Wiley Online Library: 331–55.</p>
</div>
<div id="ref-pearl2009causality">
<p>Pearl, Judea. 2009. <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press.</p>
</div>
<div id="ref-robins2004">
<p>Robins, James M. 2004. “Optimal Structural Nested Models for Optimal Sequential Decisions.” In <em>Proceedings of the Second Seattle Symposium in Biostatistics: Analysis of Correlated Data</em>, edited by D. Y. Lin and P. J. Heagerty, 189–326. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4419-9076-1_11" class="uri">https://doi.org/10.1007/978-1-4419-9076-1_11</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, Richard S, Andrew G Barto, and others. 1998. <em>Introduction to Reinforcement Learning</em>. Vol. 135. MIT press Cambridge.</p>
</div>
<div id="ref-moodie2013">
<p>“Temporary.” n.d.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
<div id="ref-vdl2011targeted">
<p>van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-vdl2018targeted">
<p>van der Laan, Mark J, and Sherri Rose. 2018. <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-vanderLaanLuedtke15">
<p>van der Laan, M. J, and A. Luedtke. 2015. “Targeted Learning of the Mean Outcome Under an Optimal Dynamic Treatment Rule.” <em>Journal of Causal Inference</em> 3 (1): 61–95.</p>
</div>
<div id="ref-cvtmle2010">
<p>Zheng, W., and M. J van der Laan. 2010. “Asymptotic Theory for Cross-validated Targeted Maximum Likelihood Estimation.” <em>U.C. Berkeley Division of Biostatistics Working Paper Series.</em></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tmle3-targeted-learning-framework.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stochastic-treatment-regimes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/acic2019-workshop/edit/master/04-tmle3mopttx.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["handbook.pdf", "handbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
